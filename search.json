[
  {
    "objectID": "computer_science.html",
    "href": "computer_science.html",
    "title": "Rcambier‚Äôs Blog",
    "section": "",
    "text": "Easy way to queue a job in the terminal\n\n\n\n\n\nHow to use background jobs to quickly queue a job after a running command\n\n\n\n\n\nApr 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPython multiprocessing with loading bar\n\n\n\n\n\nCreate a nice multiprocessing logic with a loading bar\n\n\n\n\n\nApr 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nCython for fast python\n\n\n\n\n\nTrying out cython\n\n\n\n\n\nOct 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian riddle\n\n\n\n\n\nThe typical bayesian interview question, solved with PyMC3\n\n\n\n\n\nSep 27, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Thanks for passing by :)"
  },
  {
    "objectID": "posts/2018-12-10-rope.html",
    "href": "posts/2018-12-10-rope.html",
    "title": "Lighter",
    "section": "",
    "text": "You have a lighter and two lengths of rope that burn for exactly 1 hour each. They do not burn consistently so you cannot, for example, cut one in half and burn that. How can you use these two ropes to measure exactly 45 minutes ?\n\nHover to show the answer.\n\n\n\nLight one rope from both ends and one rope from one end.\nAfter 30 minutes, the first rope will be finished burning.\nYou have 30 minutes left of burning on the second rope. Light the second end of that rope so that it will burn for 15 minutes instead of 30 minutes.\nThe total burning time will be 45 minutes."
  },
  {
    "objectID": "posts/2018-12-26-4-pieces.html",
    "href": "posts/2018-12-26-4-pieces.html",
    "title": "4 Pieces",
    "section": "",
    "text": "There are 4 coins on a quadrant. They are not visible to you.\n{:width=‚Äú300px‚Äù}\nIf the 4 coins are heads or the four coins are tails, the center light will light up. You have 5 turns to make the center light light up. Each turn, you can choose 2 coins that you reveal (you see if they are heads or tails) and you can flip any of them if you want to. You can flip one, both, or none. After that, they are hidden back from you. After each turn, we spin the quadrant so fast that you can not keep track of wich two coins you interacted with last turn. If the coins are randomly initialized, what strategy allows you to be sure to turn the light on ?\n\nHover to show the answer.\n\n\n\nBy following this strategy, you will be sure to turn on the light."
  },
  {
    "objectID": "posts/2018-12-25-broken-plane.html",
    "href": "posts/2018-12-25-broken-plane.html",
    "title": "Broken plane",
    "section": "",
    "text": "Imagine a 747 is sitting on a conveyor belt, as wide and long as a runway. The conveyor belt is designed to [try to ?] exactly match the speed of the wheels, moving in the opposite direction. Can the plane take off? (different answers if you consider the words between brackets or not)\n\nHover to show the answer.\n\n\n\nYes. (Please don‚Äôt hit me).\nIf by speed of the wheels, we consider the speed of the the center of the wheels relative to the ground, then the answer is yes. The plane doesn‚Äôt use its wheel to take off. The motor thrust don‚Äôt care that the wheels are spinning or sliding or doing nothing. You have to realize that this is not like a car, the force is no pushing at the wheels, the force making the plane go forward is pushing where the motors are attached. So the wheels are just going to spin faster, and if the conveyor belt is long enough, the plane will take off.\nAnother way to see it is that the wheels are just there to keep the plane above the ground. They can spin at the speed they want, if the plane can move forward, it will take off.\nNow, if by speed of the wheels, we consider the angular speed of the wheels then it gets complicated. This would basically be like saying ‚Äúthe conveyor belt does everything possible to make the plane not move‚Äù.\nAt that point, if the plane moves forward, the problem becomes mathematically impossible. When the plane is not moving, the angular speed is 0. When the plane moves, the angular speed is, let‚Äôs say, 10 ms/s. Now if the conveyor belt matches that speed, it will spin at 10m/s. But since the plane is still moving forward, this will make the wheels go at 20m/s (We consider here that the friction of the wheels is 0, they don‚Äôt affect the plane). So now the conveyor belt will have to match that speed. If the conveyor belt just tries to match the angular speed of the wheels, it will just go faster and faster up to infinity. If the friction of the wheels is 0, you could still argue in that situation that the plane will take off, with the wheel spinning infinitely fast in the opposite direction.\nIf the friction of the wheels is bigger than 0, you could argue that the threadmill manages to stop the plane.\nIf instead of the treadmill ‚Äútrying to match‚Äù you have the treadmill ‚Äúexactly matching‚Äù the speed of the wheels, than this becomes impossible. As soon as the plane starts going forward, this becomes an impossible situation because the treadmill has to match a speed that grows as it would try to match it.\nNote that it doesn‚Äôt mean that the plane does not take off, but that the problem is mathematically impossible.\nYou can have a look at the excellent explanation from Randall Monroe on this problem here"
  },
  {
    "objectID": "posts/2018-12-26-simple-maths.html",
    "href": "posts/2018-12-26-simple-maths.html",
    "title": "Simple Maths",
    "section": "",
    "text": "How do you make 26 by using 5, 5, 5 and 1. You have to use each exactly once. You can use the basic math operations (+, -, *, /, (, ) ).\n\nHover to show the answer.\n\n\n\n(5 + (1/5))*5 = 26"
  },
  {
    "objectID": "posts/2018-12-10-magic-loop.html",
    "href": "posts/2018-12-10-magic-loop.html",
    "title": "Magic logic",
    "section": "",
    "text": "At a party last night, my friends Alice and Bob did a magic trick. Any ideas how it worked?\nAlice shuffled a pack of cards, and asked me to take five. I looked at them. She put the rest of the pack down on the table. Alice asked for my cards. She had a look at them. She gave four of them to Bob (he was across the table), and the fifth back to me. Bob looked at the four cards for a while. Then Bob looked at me, and named the card I was holding. He was right. I‚Äôm quite sure he couldn‚Äôt have seen it (we weren‚Äôt sitting by a mirror).\nThey did the trick again later to someone else. I watched for funny business. Alice didn‚Äôt say anything to Bob, so I don‚Äôt think they have a code. Also Alice is famously clumsy, so I doubt it was sleight of hand.\n\n\n\ngender\n\n\n\nHover to show the answer.\n\n\n\nWhen Alice had the five cards in hands, she needs to find a way to select one, and communicate which one she selected to Bob by showing Bob the four other cards.\nHere is the way to do that: - In the five cards, there are always two of the same suite. - Alice is going to chose one of those two and tell which one to Bob using the 3 remaining cards. - The thing to notice is: when having two cards of the same suite, they are alway 6 cards appart in some direction. For example 7-heart and 9-heart are 2 apart. But queen-spade and 2-spade are 3 apart: queen-king-ace-2. - Now since there are 12 different cards per suite, two cards are always going to be maximum 6 steps away from each other. - So the way to do it is: from the two cards of the same suite, pick the one that allows reaching the other by adding up to 6 steps. So with 7-heart and 9-heart, pick 7-heart. With 2-spade and queen-spade, pick queen-spade. - Let‚Äôs continue in the case where Alice picked the queen-spade. - Alice gives back to Bob 3 cards + the queen-spade on top. And Bob has to guess 2-spade. - The goal is now to make the number ‚Äò3‚Äô with the 3 remaining random cards. That way Bob knows he has to add ‚Äò3‚Äô to the queen-spade, which will give him the 2-spade. - This is easily doable. With 3 cards you can make 6 combinations. Just assign each of them to a number. - Here is a way to do this: The 3 cards can always be ordered by value. So you can have card-1, card-2 and card-3. The six orders could be: [1-2-3, 1-3-2, 2-1-3, 2-3-1, 3-1-2, 3-2-1]. Here we would select the order ‚Äò2-1-3‚Äô to represent the number 3. - So Bob sees the queen-spade, and then 3 cards that represent the number ‚Äò3‚Äô. He can now guess the 2-spade."
  },
  {
    "objectID": "posts/2018-12-25-cats-and-mouses.html",
    "href": "posts/2018-12-25-cats-and-mouses.html",
    "title": "Cats and mouses",
    "section": "",
    "text": "If 3 cats take 3 minutes to catch 3 mouses, how many cats are needed to catch 100 mouses in 100 minutes ?\n\n\n\ngender\n\n\n\nHover to show the answer.\n\n\n\nYou also need 3 cats.\nIf 3 cats take 3 minutes to catch 3 mouses, it means each cat takes 3 minutes to catch its own mouse. In 100 minutes, each of those cats will catch on average 33.333‚Ä¶ mouses. So 3 cats are enough to catch 100 mouses in 100 minutes.\nSaid differently, if 3 cats take 3 minutes to catch 3 mouses, these 3 cats catch 1 mouse/minute. At that rate, they will capture 100 mouses in 100 minutes."
  },
  {
    "objectID": "posts/2021-01-05-newcomb.html",
    "href": "posts/2021-01-05-newcomb.html",
    "title": "The newcomb poison",
    "section": "",
    "text": "You are put into a room with a suitcase and a flask of poison.\nThe suitcase has been filled either with 1 million dollar or with worthless magazines.\nThe poison is bad enough that you would never drink it for free, but it will not kill you. You would definitely drink it for 1 million dollar. It is going to give you a few days of headaches and feeling very ill.\nWether the suitace contains money or magazines depends on a prediction that was made by a machine. - If the machine predicts that you will drink the poison, the suitcase will contain the money - If the machine predicts that you will not drink the poison, the suitcase will be full of magazines.\nFrom more than 10 000 past experiments, we know that the machine has 95% accuracy on both cases.\nYou are now in the room. The prediction has been made on you and the suitcase has been filled accordingly.\nDo you drink the poison and open the suitcase ? Or do you open the suitcase right away ?\nNote: This is taken from Mr Phi video serie where he discusses the NewComb Paradox\n\nHover to show the answer.\n\n\n\nMy understanding at this time (early 2021) is the following: As a human being, you can not make a plan and stick to it. This would mean inhibiting your rationality. This is something that, unfortunately, we can not do.\nSo even if you 100% plan to drink the poison, you will doubt that decision once you are in the room.\nI think I am not able to stick to a plan of drinking the poison. Therefore, I think I would not drink the poison and go straight for the suitcase.\nThis is concluding that there is no way for me to win the money.\nSomeone that could inhibit his rationality and stick to a plan could win the money. Someone not thinking about it too much and going for the poison could win the money. But as soon as you think too much about it, you understand that there is no way you actually drink the poison once you are in the room.\nTo help explain it: - The extreme case of a transparent suitcase. In that case, everyone will have to answer that they don‚Äôt drink the poison. - The extreme case of someone that can turn off his rationality. That person will be able to stick to his plan of drinking and actually win the money. But this is not possible for a normal human being. - Look for the ‚Äúfatality of rationality‚Äù for this concept."
  },
  {
    "objectID": "posts/2021-09-16-svd.html",
    "href": "posts/2021-09-16-svd.html",
    "title": "SVD",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom scipy.linalg import eig\n\nraw = pd.read_csv(\"https://raw.githubusercontent.com/smanihwr/ml-latest-small/master/ratings.csv\")\nuser_item_interactions = raw.pivot(values=\"rating\", columns=\"movieId\", index=\"userId\")\nuser_item_interactions = user_item_interactions.fillna(0)\nA = np.array([\n              [5,5,0,1],\n              [5,5,0,0],\n              [0,1,5,5],\n              [0,0,5,5],\n              [0,0,3,5]\n])\n\n# Get the singular vectors of V from the eigenvectors of the covariance matrix\nV_eigen_values, V_unordered = np.linalg.eig(A.T @ A) \n# We need to sort them by the magnitude of the eigenvalues\nidx_V = np.argsort(V_eigen_values)[::-1] \nV = V_unordered[:,idx_V]\n\n# Compute the singular vectors of U. We could also use the eingenvectors, but we need to base it on V to have the correct vector directions.\n# U_eigen_values, U = np.linalg.eig(A @ A.T) this is similar but leads to incorrect directions for the eigenvectors\nU = A @ V / np.linalg.norm(A @ V, axis=0)\n\n# The matrix D is the square root of the eigenvalues.\nD = np.sqrt(np.around(V_eigen_values[idx_V], decimals=10))\nnp.around(np.matrix(U) @ np.diag(D) @ np.matrix(V.T), decimals=1)\n\narray([[ 5.,  5., -0.,  1.],\n       [ 5.,  5.,  0., -0.],\n       [ 0.,  1.,  5.,  5.],\n       [-0.,  0.,  5.,  5.],\n       [ 0., -0.,  3.,  5.]])\nU_, D_, Vt_ = np.linalg.svd(A)\nnp.around(np.matrix(U_) @ np.vstack((np.diag(D_), np.zeros((len(Vt_))))) @ np.matrix(Vt_), decimals=1)\n\narray([[ 5.,  5., -0.,  1.],\n       [ 5.,  5., -0., -0.],\n       [-0.,  1.,  5.,  5.],\n       [-0.,  0.,  5.,  5.],\n       [-0.,  0.,  3.,  5.]])"
  },
  {
    "objectID": "posts/2021-09-16-svd.html#truncated-svd",
    "href": "posts/2021-09-16-svd.html#truncated-svd",
    "title": "SVD",
    "section": "Truncated SVD",
    "text": "Truncated SVD\nTruncate the SVD to 2 components by only keeping the two bigest eigenvalues\n\nnp.matrix(U[:, :2])\n\nmatrix([[-0.23093819, -0.66810948],\n        [-0.16863574, -0.68636674],\n        [-0.59892473,  0.13274366],\n        [-0.57986295,  0.20070102],\n        [-0.47252267,  0.15693514]])\n\n\n\nnp.around(np.matrix(U[:, :2]) @ np.diag(D[:2]) @ np.matrix(V[:,:2].T), decimals=1)\n\narray([[ 5. ,  5. ,  0.3,  0.8],\n       [ 5. ,  5. , -0.2,  0.2],\n       [ 0.3,  0.7,  4.7,  5.3],\n       [-0.2,  0.2,  4.7,  5.3],\n       [-0.1,  0.2,  3.8,  4.3]])"
  },
  {
    "objectID": "posts/2023-10-20-bayesian-inference-from-scratch.html",
    "href": "posts/2023-10-20-bayesian-inference-from-scratch.html",
    "title": "Bayesian Inference from scratch in Python",
    "section": "",
    "text": "I‚Äôve always struggled to understand how Bayesian inference worked, what was the difference between events and random variables, why sampling was need, how it was used for regression and more.\nAs always, to start understanding things better, I code them from scratch in Python :)\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import binom\nfrom scipy.stats import uniform, bernoulli\n\nfrom scipy.stats import beta\nfrom scipy.stats import betabinom\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport numpy as np\nrng = np.random.default_rng()\n\nplt.rcParams[\"figure.figsize\"] = (5,3)\nhttps://app.diagrams.net/#G1YM2D2ebDdepO6ZMQ2p65xGcgHqq8Ph49"
  },
  {
    "objectID": "posts/2023-10-20-bayesian-inference-from-scratch.html#you-have-some-prior-knowledge-about-the-probability-of-each-random-event",
    "href": "posts/2023-10-20-bayesian-inference-from-scratch.html#you-have-some-prior-knowledge-about-the-probability-of-each-random-event",
    "title": "Bayesian Inference from scratch in Python",
    "section": "1. You have some prior knowledge about the probability of each random event",
    "text": "1. You have some prior knowledge about the probability of each random event\nThere is a set of events for which we have some a-priori knowledge of probabilities.\nFor example, let‚Äôs take the following situation:\n\nYou‚Äôre about to get on a plane to Seattle. You want to know if you should bring an umbrella. You call a friendof yours who live there and ask if it‚Äôs raining. Your friend has a 2/3 chance of telling you the truth and a 1/3 chance of messing with you by lying. He tells you that ‚ÄúYes‚Äù it is raining. What is the probability that it‚Äôs actually raining in Seattle?\n\nWe are interested in the probabilities of 2 events: - Event ‚ÄúRain‚Äù - Event ‚ÄúNoRain‚Äù\nYou might not know anything about the probabilities of each of these events, but you might have some a-priori. Either way, you can express your current belief in probabilities of each of those events.\nWe can visualize our current, our ‚Äúprior‚Äù knowledge with a graph like the following\n\n\n\nimage.png\n\n\n\nprobabilities = pd.DataFrame({\n    'events': ['Rain', 'NoRain'],\n    'prior_knowledge': [0.5, 0.5],\n});\n\nprobabilities\n\n\n  \n    \n\n\n\n\n\n\nevents\nprior_knowledge\n\n\n\n\n0\nRain\n0.5\n\n\n1\nNoRain\n0.5"
  },
  {
    "objectID": "posts/2023-10-20-bayesian-inference-from-scratch.html#you-have-an-additional-knowledge-the-relationship-between-another-event-happening-and-the-probabilities-of-events-rain-and-norain",
    "href": "posts/2023-10-20-bayesian-inference-from-scratch.html#you-have-an-additional-knowledge-the-relationship-between-another-event-happening-and-the-probabilities-of-events-rain-and-norain",
    "title": "Bayesian Inference from scratch in Python",
    "section": "2. You have an additional knowledge: the relationship between another event happening and the probabilities of events Rain and NoRain",
    "text": "2. You have an additional knowledge: the relationship between another event happening and the probabilities of events Rain and NoRain\nNow, there is another event happening: you call a friend and he tells you that it is raining. And you know (maybe from past experiences) that this friend is lying 1/3rd of the time.\nThe event happening: ‚ÄúCalling a 1/3rd lying friend that tells you it is raining‚Äù\nNote something: this is another event, which happens ‚Äúon top‚Äù of the event raining/not raninig. But there is a relationship between these events that (for some reason) you can exactly express mathematically.\nThe trick is to apply this relationship, and understand the likelihood of the event ‚ÄúCalling a 1/3rd lying friend that tells you it is raining‚Äù in each of the initial events: - Rain: in this situation there is 2/3 chances of the friend saying that it rains - NoRain: in this situation there is 1/3 chances of the friend saying that it rains\nSo we now have a sytem to tell, in each situtation ‚ÄúRain‚Äù and ‚ÄúNoRain‚Äù of the initial event, what are the chances of observing this new event of the friend mentioning rain.\n\nThe event ‚ÄúCalling a 1/3rd lying friend that tells you it is raining‚Äù is called the data\nYour prior expcations in the events ‚ÄúRain‚Äù/‚ÄúNoRain‚Äù is called the prior\n\nI like to visualize this on top of the previous graph:\nThis drives home for me the fact that we have a pre-existing belief in the probabilities for each event, and that the new event happening has likelihoods of happening that can be different in each of the initial events.\n\n\n\nimage.png\n\n\n\n# We observed the data when calling our friend\nlikelihood = pd.DataFrame({\\\n    'events': ['Rain', 'NoRain'],\n    'likelihood': [2/3, 1/3], # We use our system, here logical thinking from what is said in the prompt, to link the observations to each event\n});\n\nlikelihood\n\n\n  \n    \n\n\n\n\n\n\nevents\nlikelihood\n\n\n\n\n0\nRain\n0.666667\n\n\n1\nNoRain\n0.333333"
  },
  {
    "objectID": "posts/2023-10-20-bayesian-inference-from-scratch.html#now-you-combine-both-to-answer-the-question-what-are-the-chances-of-raining-a-if-eventfriend-telling-rain-happened",
    "href": "posts/2023-10-20-bayesian-inference-from-scratch.html#now-you-combine-both-to-answer-the-question-what-are-the-chances-of-raining-a-if-eventfriend-telling-rain-happened",
    "title": "Bayesian Inference from scratch in Python",
    "section": "3. Now you combine both to answer the question ‚ÄúWhat are the chances of Raining (A) if event‚ÄùFriend telling rain‚Äù happened ?",
    "text": "3. Now you combine both to answer the question ‚ÄúWhat are the chances of Raining (A) if event‚ÄùFriend telling rain‚Äù happened ?\nWhat you can tell looking at the graph, is the chances of the friend saying ‚Äúrain‚Äù if A or saying ‚Äúrain‚Äù if B.\nTo apply Bayes Rule and answer the question, you need to restrict the events to only the ‚Äú‚Äúrain‚Äù - friend‚Äù events\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\nbayes = pd.concat([probabilities, likelihood.drop(columns=['events'])], axis=1)\nbayes\n\n\n  \n    \n\n\n\n\n\n\nevents\nprior_knowledge\nlikelihood\n\n\n\n\n0\nRain\n0.5\n0.666667\n\n\n1\nNoRain\n0.5\n0.333333\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nbayes['posterior_unnormalized'] = bayes['prior_knowledge'] * bayes['likelihood']\nbayes\n\n\n  \n    \n\n\n\n\n\n\nevents\nprior_knowledge\nlikelihood\nposterior_unnormalized\n\n\n\n\n0\nRain\n0.5\n0.666667\n0.333333\n\n\n1\nNoRain\n0.5\n0.333333\n0.166667\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\n\nimage.png\n\n\n\nbayes['posterior'] = bayes['posterior_unnormalized'] / bayes['posterior_unnormalized'].sum()\nbayes\n\n\n  \n    \n\n\n\n\n\n\nevents\nprior_knowledge\nlikelihood\nposterior_unnormalized\nposterior\n\n\n\n\n0\nRain\n0.5\n0.666667\n0.333333\n0.666667\n\n\n1\nNoRain\n0.5\n0.333333\n0.166667\n0.333333\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\nFinally, this way, we can answer that the probabilities of rain (A) given that the friend said that it was raining is 66%."
  },
  {
    "objectID": "posts/2021-09-27-bayesian-seattle.html",
    "href": "posts/2021-09-27-bayesian-seattle.html",
    "title": "Bayesian riddle",
    "section": "",
    "text": "Here is a statement I saw multiple times online. I also received it once during a Data Science interview. I was not sure at first how to fully solve it."
  },
  {
    "objectID": "posts/2021-09-27-bayesian-seattle.html#using-the-bayes-formula",
    "href": "posts/2021-09-27-bayesian-seattle.html#using-the-bayes-formula",
    "title": "Bayesian riddle",
    "section": "Using the bayes formula",
    "text": "Using the bayes formula\nWe want to get p(rain | 3xYes)\n\np(rain | 3xYes) = (p(3xYes | rain ) * p(rain)) / P(3xYes)        # Bayes formula\n\np(rain | 3xYes) = (((2/3)^3 ) * p(rain)) / P(3xYes)              # If it rains, there is (2/3)^3 chances of having 3xYes        \n\np(rain | 3xYes) = ((8/27) * p(rain)) / P(3xYes)\n\np(rain | 3xYes) = ((8/27) * p(rain)) / (P(3xYes | rain) p(rain) + P(3xYes | not_rain) p (not_rain))      # The chances of having 3xYes are the chances \n                                                                                                         # of having it when it rains, added to chances \n                                                                                                         # of having it when it does not rain\n\np(rain | 3xYes) = ((8/27) * p(rain)) / (8/27) p(rain) + (1/27) p (not_rain))\n\np(rain | 3xYes) = ((8) * p(rain)) / (8 p(rain) + ( 1- p (rain) ))\n\np(rain | 3xYes) = ((8) * p(rain)) / (7 p(rain) + ( 1 ))"
  },
  {
    "objectID": "posts/2021-09-27-bayesian-seattle.html#choosing-a-prior",
    "href": "posts/2021-09-27-bayesian-seattle.html#choosing-a-prior",
    "title": "Bayesian riddle",
    "section": "Choosing a prior",
    "text": "Choosing a prior\nNow that you did that, you are left with a formula that depends on the probability that it rains. You can choose a prior for what you consider your a-priori belief on the chances of raining, and plug it in.\nFor example, if you chose the prior p(rain) = 0.5, it gives.\np(rain | 3xYes) = 0.8888"
  },
  {
    "objectID": "posts/2018-12-10-prisoners-trees.html",
    "href": "posts/2018-12-10-prisoners-trees.html",
    "title": "Prisoners and trees",
    "section": "",
    "text": "Here is a logic puzzle that depends on the game theory concept of common knowledge.\nCan you figure it out? Alice and Bob are taken prisoners by an evil logician. They are given one chance to be set free. Alice and Bob are placed in cells that have a view of a courtyard with trees. There are 20 trees in all, of which Alice sees 12 and Bob sees 8. Neither prisoner knows how many trees the other sees. But each prisoner is told the trees are partitioned between them: together they see all the trees, but individually no tree is seen by both of them.\n\n\n\nprisoners trees\n\n\nThey have to figure out the total number of trees, but they are not allowed to communicate with each other. Each day the logician visits Alice in her cell and asks, ‚ÄúAre there 18 or 20 trees in total?‚Äù Alice has two choices: she can guess or pass. If Alice passes, then the logician visits Bob in his cell and asks the same question. Bob also can guess or pass. If Bob passes, then the logician retires for the night asks and repeats asking the questions the next day. Both prisoners know the procedure of how the logician is asking questions. There are consequences to guessing. If either person guesses incorrectly, then they are both trapped forever. If either person guesses correctly, however, then they are both set free immediately. Obviously they could guess and have a 50% chance. But can they do better? Is there a way they can escape with certainty?\n\nHover to show the answer.\n\n\n\nThis is a complex one. Answer is coming."
  },
  {
    "objectID": "posts/2021-01-05-scoundrels.html",
    "href": "posts/2021-01-05-scoundrels.html",
    "title": "Three‚Äôs a Crowd",
    "section": "",
    "text": "From: http://www.twinbear.com/riddles.html\n(Clayton Lewis) After solving the riddle of the three wise folks, three scoundrels claim to be the smartest in the country. So you decide to give them a challenge. Suspecting that the thing they care about most is money, you give them $100 and tell them they are to divide this money observing the following rule: they are to discuss offers and counter-offers from each other and then take a vote. Majority vote wins. Sounds easy enough‚Ä¶ now the question is, assuming each person is motivated to take the largest amount possible, what will the outcome be?\nNote: careful‚Ä¶ if the answer were that they split it 50% / 50% / 0%, or 1/3 / 1/3 / 1/3, it wouldn‚Äòt be a riddle!\nNote: careful‚Ä¶ 96.6523544 % of people who send answers to this have not thought about it for even 1 minute. I guarantee you won‚Äòt solve it in a minute. (96.6523544% of the time this guarantee is correct.)\n\nHover to show the answer.\n\n\n\nI can not solve this one so far. If you can please share your answer!"
  },
  {
    "objectID": "posts/2020-04-19-reverse-number.html",
    "href": "posts/2020-04-19-reverse-number.html",
    "title": "Reversed number",
    "section": "",
    "text": "Can you find a 4 digits number that gets reversed when multiplied by 4?\nFor example: if 4*1234 == 4321, 1234 would be an answer. Unfortunately, that is not the case. All digits need to be different.\n\nHover to show the answer."
  },
  {
    "objectID": "posts/2018-12-10-gender.html",
    "href": "posts/2018-12-10-gender.html",
    "title": "Gender",
    "section": "",
    "text": "A lady has two children. One is a boy. What are the chances of the other child also being a boy?\n\n\n\ngender\n\n\n\nHover to show the answer.\n\n\n\nThis one is more complicated that I previously thought‚Ä¶"
  },
  {
    "objectID": "posts/2021-09-18-boosting-trees.html",
    "href": "posts/2021-09-18-boosting-trees.html",
    "title": "Gradient Boosting trees",
    "section": "",
    "text": "from sklearn.datasets import load_wine, load_breast_cancer, load_boston\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score, mean_absolute_error\nfrom sklearn.ensemble._gb_losses import BinomialDeviance\n\n\nraw = load_boston(return_X_y=True)\n\nX = pd.DataFrame(raw[0])\ny = pd.DataFrame(raw[1])\n\ninitial_predictions = [y.mean()[0]] * len(y)\n\n\nprint(\"Error with mean: \", mean_absolute_error(y, initial_predictions))\n\nlearning_rate = 0.3\n\n# Let's build some trees !\npredictions_so_far = initial_predictions\ngradient_of_loss = (y.values.reshape(-1) - predictions_so_far)\ntrees = []\nfor i in range(5): \n\n  # Train a tree on the latest residuals\n  tree = DecisionTreeRegressor(max_depth=1)\n  tree.fit(X, gradient_of_loss)\n  trees.append(tree)\n\n  # Compute the predictions of the trees\n  predictions_so_far = predictions_so_far + learning_rate * tree.predict(X).reshape(-1) # Each tree tries to predict the error. \n\n  # Get the new residuals. This is what we fit the next tree on\n  # Residuals are the gradient of the loss with respect to the previous trees predictions. \n  # In this case the loss is MSE: \n  # loss = (y_hat - y) ** 2\n  # loss_gradient_with_respect_to_y = - 2 * (y_hat - y) = 2 * (y - y_hat)\n  gradient_of_loss =  2* (y.values.reshape(-1) - predictions_so_far)\n\nprint(\"Error with boosting: \", mean_absolute_error(y, predictions_so_far))\n\nError with mean:  6.647207423956011\nError with boosting:  3.3369627690621475\n\n\n\n\n\n\nfrom sklearn.datasets import load_wine, load_breast_cancer\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score\n\n\ndef update_lead_values(tree): \n    \n\n\nraw = load_breast_cancer(return_X_y=True)\n\nX = pd.DataFrame(raw[0])\ny = pd.DataFrame(raw[1])\n\np = y.mean()[0]\ninitial_predictions = np.array([np.log(p/(1-p))] * len(y))# Initial prediction is logodds of y\nprint(\"Initial score: \", f1_score(y, (initial_predictions &gt; 0.5) *1))\n\n\nlearning_rate = 0.3\n\ndef sigmoid(x): \n  return 1 / (1 + np.exp(-x))\n\ny_hat = sigmoid(initial_predictions)\ngradient_of_loss = y_hat - y.values.reshape(-1)\n\n\ntrees = []\npredictions_so_far = initial_predictions\nfor i in range(5): \n\n  # Train a tree on the latest residuals\n  tree = DecisionTreeRegressor(max_depth=1)\n  tree.fit(X, gradient_of_loss)\n\n  # TODO: Here you need to update the values of the tree leaves\n  # to equal a specific value each. \n\n  trees.append(tree)\n\n  # Compute the predictions of the trees\n  predictions_so_far = predictions_so_far - learning_rate * tree.predict(X).reshape(-1) \n\n  # The gradient of the loss with respect to y_hat\n  # is y_hat - y. Neat.\n  y_hat = sigmoid(predictions_so_far)\n  gradient_of_loss =  y_hat - y.values.reshape(-1)\n\nprint(\"Score with boosting: \", f1_score(y, 1 * (sigmoid(predictions_so_far) &gt; 0.5)))\n\nInitial score:  0.7710583153347732\nScore with boosting:  0.922279792746114\n\n\n\ntrees[0].tree_\n\n&lt;sklearn.tree._tree.Tree at 0x7faeec0f2180&gt;"
  },
  {
    "objectID": "posts/2021-09-18-boosting-trees.html#regression",
    "href": "posts/2021-09-18-boosting-trees.html#regression",
    "title": "Gradient Boosting trees",
    "section": "",
    "text": "from sklearn.datasets import load_wine, load_breast_cancer, load_boston\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score, mean_absolute_error\nfrom sklearn.ensemble._gb_losses import BinomialDeviance\n\n\nraw = load_boston(return_X_y=True)\n\nX = pd.DataFrame(raw[0])\ny = pd.DataFrame(raw[1])\n\ninitial_predictions = [y.mean()[0]] * len(y)\n\n\nprint(\"Error with mean: \", mean_absolute_error(y, initial_predictions))\n\nlearning_rate = 0.3\n\n# Let's build some trees !\npredictions_so_far = initial_predictions\ngradient_of_loss = (y.values.reshape(-1) - predictions_so_far)\ntrees = []\nfor i in range(5): \n\n  # Train a tree on the latest residuals\n  tree = DecisionTreeRegressor(max_depth=1)\n  tree.fit(X, gradient_of_loss)\n  trees.append(tree)\n\n  # Compute the predictions of the trees\n  predictions_so_far = predictions_so_far + learning_rate * tree.predict(X).reshape(-1) # Each tree tries to predict the error. \n\n  # Get the new residuals. This is what we fit the next tree on\n  # Residuals are the gradient of the loss with respect to the previous trees predictions. \n  # In this case the loss is MSE: \n  # loss = (y_hat - y) ** 2\n  # loss_gradient_with_respect_to_y = - 2 * (y_hat - y) = 2 * (y - y_hat)\n  gradient_of_loss =  2* (y.values.reshape(-1) - predictions_so_far)\n\nprint(\"Error with boosting: \", mean_absolute_error(y, predictions_so_far))\n\nError with mean:  6.647207423956011\nError with boosting:  3.3369627690621475"
  },
  {
    "objectID": "posts/2021-09-18-boosting-trees.html#classification",
    "href": "posts/2021-09-18-boosting-trees.html#classification",
    "title": "Gradient Boosting trees",
    "section": "",
    "text": "from sklearn.datasets import load_wine, load_breast_cancer\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score\n\n\ndef update_lead_values(tree): \n    \n\n\nraw = load_breast_cancer(return_X_y=True)\n\nX = pd.DataFrame(raw[0])\ny = pd.DataFrame(raw[1])\n\np = y.mean()[0]\ninitial_predictions = np.array([np.log(p/(1-p))] * len(y))# Initial prediction is logodds of y\nprint(\"Initial score: \", f1_score(y, (initial_predictions &gt; 0.5) *1))\n\n\nlearning_rate = 0.3\n\ndef sigmoid(x): \n  return 1 / (1 + np.exp(-x))\n\ny_hat = sigmoid(initial_predictions)\ngradient_of_loss = y_hat - y.values.reshape(-1)\n\n\ntrees = []\npredictions_so_far = initial_predictions\nfor i in range(5): \n\n  # Train a tree on the latest residuals\n  tree = DecisionTreeRegressor(max_depth=1)\n  tree.fit(X, gradient_of_loss)\n\n  # TODO: Here you need to update the values of the tree leaves\n  # to equal a specific value each. \n\n  trees.append(tree)\n\n  # Compute the predictions of the trees\n  predictions_so_far = predictions_so_far - learning_rate * tree.predict(X).reshape(-1) \n\n  # The gradient of the loss with respect to y_hat\n  # is y_hat - y. Neat.\n  y_hat = sigmoid(predictions_so_far)\n  gradient_of_loss =  y_hat - y.values.reshape(-1)\n\nprint(\"Score with boosting: \", f1_score(y, 1 * (sigmoid(predictions_so_far) &gt; 0.5)))\n\nInitial score:  0.7710583153347732\nScore with boosting:  0.922279792746114\n\n\n\ntrees[0].tree_\n\n&lt;sklearn.tree._tree.Tree at 0x7faeec0f2180&gt;"
  },
  {
    "objectID": "posts/2021-09-19-word-embeddings.html",
    "href": "posts/2021-09-19-word-embeddings.html",
    "title": "Word embedding",
    "section": "",
    "text": "We will use the following method to build simple word embeddings.\nWe create a matrix where we put the co-occurences of all the words.\nWe factorize that matrix.\n\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\n\nsentences = [\n    \"a dog is a sweet animal\",\n    \"a cat is a mean beast\", \n    \"a human is a different creature\",\n    \"a cat is a nice pet\",\n    \"a dog is a nice pet also\"\n]\n\n\nfrom collections import defaultdict\nfrom itertools import product, combinations\n\nNij_counts = defaultdict(int)\n\nN = 1\nk = 5 # The window size\nwindow_size = 2\n\nvocab = set()\nfor sentence in sentences: \n    for idx_a, word_a in enumerate(sentence.split(\" \")): \n        start = idx_a - 1 \n        stop = idx_a + 2\n        for word_b in sentence.split(\" \")[start:stop]:\n            if word_a == word_b:\n              continue\n            Nij_counts[(word_a, word_b)] += 1\n            N += 1\n            vocab.add(word_a)\n            vocab.add(word_b)\n\nNi_counts = defaultdict(int)\nNj_counts = defaultdict(int)\nfor (i,j), N_ij in Nij_counts.items():\n  Ni_counts[ i ] += N_ij\n  Nj_counts[ j ] += N_ij\n\n\nPi = {k:v/N for k,v in Ni_counts.items()}\nPj = {k:v/N for k,v in Nj_counts.items()}\nPij = {k:v/N for k,v in Nij_counts.items()}\n\n\npmi_matrix = np.zeros((len(vocab), len(vocab)))\nfor i, word_i in enumerate(vocab): \n    for j, word_j in enumerate(vocab):\n        pmi_matrix[i][j] = np.log( Pij.get((word_i, word_j), 0) / (Pi[word_i] * Pj[word_j] ))  #- np.log(k)\n\npmi_matrix[ pmi_matrix &lt; 0] = 0\npd.DataFrame(pmi_matrix, columns=vocab, index=vocab)\n\nRuntimeWarning: divide by zero encountered in log\n  pmi_matrix[i][j] = np.log( Pij.get((word_i, word_j), 0) / (Pi[word_i] * Pj[word_j] ))  #- np.log(k)\n\n\n\n\n\n\n\n\n\ncat\ndifferent\nanimal\ndog\nsweet\nnice\nis\nbeast\npet\ncreature\nalso\na\nmean\nhuman\n\n\n\n\ncat\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.875469\n0.000000\n0.000000\n0.000000\n0.000000\n0.470004\n0.000000\n0.000000\n\n\ndifferent\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n3.178054\n0.000000\n0.470004\n0.000000\n0.000000\n\n\nanimal\n0.000000\n0.000000\n0.000000\n0.000000\n3.178054\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\ndog\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.875469\n0.000000\n0.000000\n0.000000\n0.000000\n0.470004\n0.000000\n0.000000\n\n\nsweet\n0.000000\n0.000000\n3.178054\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.470004\n0.000000\n0.000000\n\n\nnice\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n2.079442\n0.000000\n0.000000\n0.470004\n0.000000\n0.000000\n\n\nis\n1.568616\n0.000000\n0.000000\n1.568616\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.470004\n0.000000\n1.568616\n\n\nbeast\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n3.178054\n0.000000\n\n\npet\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n2.079442\n0.000000\n0.000000\n0.000000\n0.000000\n2.772589\n0.000000\n0.000000\n0.000000\n\n\ncreature\n0.000000\n3.178054\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nalso\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n2.772589\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\na\n0.000000\n0.875469\n0.000000\n0.000000\n0.875469\n0.875469\n0.875469\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.875469\n0.000000\n\n\nmean\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n3.178054\n0.000000\n0.000000\n0.000000\n0.470004\n0.000000\n0.000000\n\n\nhuman\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.875469\n0.000000\n0.000000\n0.000000\n0.000000\n0.470004\n0.000000\n0.000000\n\n\n\n\n\n\n\n\nU, sigma, Vt = np.linalg.svd(pmi_matrix)\nword_embeddings = U * sigma\n\n\npd.DataFrame(U * sigma, index=vocab)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\n\n\ncat\n-0.125528\n-0.082352\n-0.056639\n-0.154977\n1.770129e-16\n-3.472768e-16\n2.250913e-16\n-5.898841e-17\n0.103016\n0.950212\n-0.152147\n0.030330\n-5.896389e-17\n-8.886491e-18\n\n\ndifferent\n-0.036187\n-0.397648\n-0.036706\n-1.837233\n-8.467333e-01\n-5.539689e-01\n-2.376266e+00\n-2.507431e-01\n-0.210186\n-0.083805\n0.012984\n0.002184\n1.223874e-32\n6.412951e-34\n\n\nanimal\n-1.399474\n0.009853\n-1.144664\n0.061518\n-2.343343e+00\n9.292952e-01\n6.095900e-01\n8.310343e-02\n-0.019071\n-0.265032\n-0.152087\n0.016544\n-4.343979e-33\n-1.361987e-33\n\n\ndog\n-0.125528\n-0.082352\n-0.056639\n-0.154977\n-1.407845e-15\n1.465704e-16\n3.570005e-16\n5.723916e-16\n0.103016\n0.950212\n-0.152147\n0.030330\n-6.870092e-17\n8.445844e-18\n\n\nsweet\n-0.036187\n-0.397648\n-0.036706\n-1.837233\n7.382504e-02\n-8.159933e-01\n1.762162e+00\n-1.719547e+00\n-0.210186\n-0.083805\n0.012984\n0.002184\n1.219035e-32\n1.377710e-33\n\n\nnice\n-0.036590\n-2.088265\n0.044529\n0.326219\n1.017894e-15\n-1.242717e-15\n-1.230756e-15\n1.167800e-15\n0.001746\n0.082020\n-0.166661\n-0.199703\n4.333732e-31\n-4.584136e-33\n\n\nis\n-0.019859\n-0.171200\n-0.013028\n-0.387229\n-1.320726e-15\n0.000000e+00\n0.000000e+00\n0.000000e+00\n2.720756\n-0.141082\n0.018090\n0.002996\n-4.170220e-31\n0.000000e+00\n\n\nbeast\n-1.399474\n0.009853\n-1.144664\n0.061518\n6.504474e-01\n-2.190208e+00\n1.499843e-01\n1.220968e+00\n-0.019071\n-0.265032\n-0.152087\n0.016544\n2.060855e-32\n3.835476e-33\n\n\npet\n-2.170444\n0.092111\n2.695585\n-0.024651\n-8.828112e-16\n-1.487032e-15\n-1.830279e-15\n2.331132e-15\n-0.007100\n-0.134905\n-0.083034\n0.009092\n7.846944e-33\n2.243644e-34\n\n\ncreature\n-1.399474\n0.009853\n-1.144664\n0.061518\n1.692895e+00\n1.260913e+00\n-7.595744e-01\n-1.304071e+00\n-0.019071\n-0.265032\n-0.152087\n0.016544\n-1.541832e-32\n1.128522e-33\n\n\nalso\n-0.036888\n-2.694710\n0.065685\n0.599222\n1.951203e-15\n-1.874111e-15\n-2.216379e-15\n1.708128e-15\n-0.091009\n-0.110411\n0.133121\n0.151153\n-3.268027e-31\n1.746002e-33\n\n\na\n-1.662965\n0.007278\n-0.615354\n0.016051\n1.023148e-15\n1.197922e-15\n-3.519035e-16\n-9.126765e-16\n0.017296\n0.637811\n0.526446\n-0.059639\n-8.140555e-34\n9.449184e-34\n\n\nmean\n-0.036187\n-0.397648\n-0.036706\n-1.837233\n7.729083e-01\n1.369962e+00\n6.141042e-01\n1.970290e+00\n-0.210186\n-0.083805\n0.012984\n0.002184\n-1.920898e-32\n-7.791129e-35\n\n\nhuman\n-0.125528\n-0.082352\n-0.056639\n-0.154977\n-8.785929e-16\n8.522401e-16\n4.452092e-16\n-3.537999e-16\n0.103016\n0.950212\n-0.152147\n0.030330\n1.276648e-16\n4.406470e-19\n\n\n\n\n\n\n\n\nU_embeddings = {word: (U * sigma)[index, :] for index, word in enumerate(vocab)}\nV_embeddings = {word: Vt[index, :] for index, word in enumerate(vocab)}\n\n\n(U_embeddings['cat'] @ U_embeddings['dog'],\nU_embeddings['cat'] @ U_embeddings['is'],\nU_embeddings['cat'] @ U_embeddings['human'])\n\n(0.987348921588194, 0.22090341150415482, 0.987348921588194)"
  },
  {
    "objectID": "posts/2021-09-13-hypothesis-testing.html",
    "href": "posts/2021-09-13-hypothesis-testing.html",
    "title": "Hypothesis testing",
    "section": "",
    "text": "Hypothesis testing\n\nimport scipy as sp \nimport numpy as np \n\n\nA typical statement\nA particular brand of tires claims that its deluxe tire averages at least 50,000 miles before it needs to be replaced. From past studies of this tire, the standard deviation is known to be 8,000. A survey of owners of that tire design is conducted. From the 28 tires surveyed, the mean lifespan was 46,500 miles with a standard deviation of 9,800 miles. Using ùõº=0.05 , is the data highly inconsistent with the claim?\n\n# What we know of the population\nclaim_pop_mean = 50_000\npop_std = 8000\n\n# What we know of the sample\nn = 28\nsample_mean = 46_500\nsample_std = 9800\n\n# The chances of Type 1 error we are ready to accept\nalpha = 0.05\n\nThe question can be formulated as: - ‚ÄúCompared to the mean of that population (50_000), how crazy is the sample mean (46_500) ? With an alpha of 0.05‚Äù\nwhich becomes - ‚ÄúUsing the sample deviation of the mean of that population, how far is the sample mean ? With an alpha of 0.05‚Äù\nwhich becomes - ‚ÄúIs the sample mean further away than 1.64 times the standard error of that population ?‚Äù\n\n# 1. How far is the sample_mean from the pop_mean ?\n# H0 =&gt; pop_mean &gt;= 50_000\n# H1 =&gt; pop_mean &lt; 50_000\n\npopulation_standard_error = 8000 / np.sqrt(28) # \"If you grab a random sample mean, how is it going to variate\"\nhow_far_we_are_from_pop_mean = (46_500 - 50_000) / population_standard_error # How far is this specific sample mean from the population mean. \n\nThere are different ways to reject the null hypothesis.\nWe can look at wether we are smaller or not than 0.05.\n\nhow_far_we_are_in_z = sp.stats.norm.cdf(how_far_we_are_from_pop_mean) \nhow_far_we_are_in_z\n\n0.010305579572800304\n\n\nIn this case we are at 0.01, which means that in the distribution of sample means, we are so extreme that there is no way that the sample mean we observed actually came from the sample mean distribution that we built looking at the population.\nAnother way is to look at how far we go on the axis, not in term of percentage (like 0,05 being 5%) but in term of distance from the population mean. This would look like the following\n\nhow_far_we_are_from_pop_mean\n\n-2.315032397181517\n\n\nTo know if this is a value too extreme or not, we can compare it to how far 0.05 is on the same axis:\n\n- sp.stats.norm.ppf(0.95)\n\n-1.6448536269514722\n\n\n\n\n\nWhen you don‚Äôt have the population standard deviation\nRealistically however, you often don‚Äôt have the population standard deviation. In this case, you need to estimate it from the sample.\nDoing that is less accurate. In order to compensate a bit, we need to model the ‚Äúspread of sample means‚Äù a bit differently.\nSince normally we allow the sample mean to only go ‚Äúso far‚Äù from the population mean. We will force it to be ‚Äúeven a bit further‚Äù. The way we do this is by using a ‚Äúheavy tail‚Äù distribution for the sample mean. That way, the 0.05 mark will be further to the right or to the left, and we are forced to be a little bit more sure of ourselves before saying anything.\nLet‚Äôs use the sample problem as above, but pretend that we don‚Äôt know that the population has a standard deviation of 8000. We are forced to use the 9800 that we discovered experimentally.\n\n# What we know of the population\nclaim_pop_mean = 50_000\n# --- pop_std = 8000 # we don't know this anymore ---\n\n# What we know of the sample\nn = 28\nsample_mean = 46_500\nsample_std = 9800\n\n# The chances of Type 1 error we are ready to accept\nalpha = 0.05\n\n# 1. How far is the sample_mean from the pop_mean ?\n# H0 =&gt; pop_mean &gt;= 50_000\n# H1 =&gt; pop_mean &lt; 50_000\n\npopulation_standard_error = 9800 / np.sqrt(28) # \"If you grab a random sample mean, how is it going to variate\"\nhow_far_we_are_from_pop_mean = (46_500 - 50_000) / population_standard_error # How far is this specific sample mean from the population mean. \n\nhow_far_we_are_in_z = sp.stats.t.cdf(how_far_we_are_from_pop_mean, df=n-1) \nhow_far_we_are_in_z\n\n0.014225814767264972\n\n\nWe still reject the null hypothesis. But notice how much less confident we are ! Even if the standard deviation we sample was exactly 8000 (like the population one), we would still be less confident than if we received the standard deviation through a trustful source.\nThis is the whole point of this T student distribution !\n\n\nConfidence interval\nConfidence interval are only in the point of view of the sample we just took.\nFrom that sample, let‚Äôs just add a standard error on each side and see how far this goes.\n\nhow_much_we_allow_on_unit_normal_distrib = sp.stats.norm.ppf(0.95)\nsample_mean_standard_error = 9800 / np.sqrt(n)\nhow_much_we_allow_in_problem_domain = how_much_we_allow_on_unit_normal_distrib * sample_mean_standard_error\nhow_much_we_allow_in_problem_domain\n[46_500 - how_much_we_allow_in_problem_domain, 46_500 + how_much_we_allow_in_problem_domain]\n\n3046.311548011343"
  },
  {
    "objectID": "posts/2018-12-25-poison.html",
    "href": "posts/2018-12-25-poison.html",
    "title": "King‚Äôs poison",
    "section": "",
    "text": "In a far away land, it was known that if you drank poison, the only way to save yourself is to drink a stronger poison in the next 12 hours, which neutralizes the weaker poison.\nThe king that ruled the land wanted to make sure that he possessed the strongest poison in the kingdom, in order to ensure his survival, in any situation. So the king called the kingdom‚Äôs pharmacist and the kingdom‚Äôs treasurer, he gave each a week to make the strongest poison. Then, each would drink the other one‚Äôs poison, then his own, and the one that will survive, will be the one that had the stronger poison.\nThe pharmacist went straight to work, but the treasurer knew he had no chance, for the pharmacist was much more experienced in this field, so instead, he made up a sneaky plan to survive and make sure the pharmacist dies.\nOn the last day the pharmacist suddenly realized that the treasurer would know he had no chance, so he must have a plan. After a little thought, the pharmacist realized what the treasurer‚Äôs plan must be, and he concocted a counter plan, to make sure he survives and the treasurer dies. When the time came, the king summoned both of them. They drank the poisons as planned, the treasurer died, and the pharmacist survived.\nWhat happened ? What was the treasurer‚Äôs plan ? What was the pharmacist counter-plan ? And did the king get what he wanted ?\n\n\n\ngender\n\n\n\nHover to show the answer.\n\n\n\nThe treasurer‚Äôs plan was to drink a weak poison before going to the king‚Äôs challenge, and bring water to the challenge. In front of the king, he would drink the strong pharmacist‚Äôs poison, which would neutralize his, and then drink the water he brought. On the opposite, the pharmacist would drink water, followed by his own poison, and then die.\nWhen the pharmacist realises that, he decides to also bring water. That way he drinks water both times, and the treasurer dies from his weak poison."
  },
  {
    "objectID": "posts/2021-10-15-cython.html",
    "href": "posts/2021-10-15-cython.html",
    "title": "Cython for fast python",
    "section": "",
    "text": "A philosophy I like to follow in Python is ‚ÄúPython is slow, let‚Äôs code everything and than see if we have any bottleneck we should replace with something else‚Äù. If you find such bottlenecks, you can replace them with a faster library or another language.\nReplacing Python by Cython is one of the ways to speedup such bottlenecks.\nLet‚Äôs try to code a correlation computation function in cython and compare it to Python or Numpy.\nFirst, the easy numpy version\n\na = [1,2,3,4,5,6] * 1000\nb = [2,3,4,5,7,6] * 1000\n\n\nimport numpy as np\n\n%timeit np.corrcoef(a,b)\n\n947 ¬µs ¬± 113 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)\n\n\nNow, a simple pupre Python version.\n\ndef correlation(a_samples, b_samples): \n  a_mean = sum(a_samples) / len(a_samples)\n  b_mean = sum(b_samples) / len(b_samples)\n\n  diff_a_samples = [a - a_mean for a in a_samples]\n  diff_b_samples = [b - b_mean for b in b_samples]\n\n  covariance = sum([diff_a * diff_b for diff_a, diff_b  in zip(diff_a_samples, diff_b_samples)]) \n  variance_a = sum(diff_a ** 2 for diff_a in diff_a_samples)\n  variance_b = sum(diff_b ** 2 for diff_b in diff_b_samples)\n  correlation = covariance / (variance_a * variance_b) ** (1/2)\n  return correlation\n\n%timeit correlation(a,b)\n\n2.38 ms ¬± 85.8 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n\n\nLet‚Äôs now try to build a version in cython.\nFirst, I have to transform the Python lists in C arrays. Then I compute the values one by one, making sure that I don‚Äôt leave any Python operations.\n\n%load_ext cython\n\nThe cython extension is already loaded. To reload it, use:\n  %reload_ext cython\n\n\n\n%%cython\n\nimport cython\n\nfrom libc.stdlib cimport malloc, free\nfrom libc.math cimport sqrt\n\ndef cython_correlation(a_samples, b_samples): \n  cdef int a_len = len(a_samples)\n  cdef int b_len = len(b_samples)\n\n  # First we convert the Python lists into C arrays\n  a_samples_array = &lt;int *&gt;malloc(a_len*cython.sizeof(int))\n  if a_samples_array is NULL:\n    raise MemoryError\n  b_samples_array = &lt;int *&gt;malloc(b_len*cython.sizeof(int))\n  if b_samples_array is NULL: \n    raise MemoryError\n  \n  cdef int i = 0\n  for i in range(a_len): \n    a_samples_array[i] = a_samples[i]\n    b_samples_array[i] = b_samples[i]\n\n  # Now we can compute the correlation\n\n  # First compute the sum of the arrays\n  cdef int a_sum = 0\n  for i in range(a_len):\n    a_sum += a_samples_array[i]\n\n  cdef int b_sum = 0\n  for i in range(a_len):\n    b_sum += b_samples_array[i]\n\n  # Then we can compute the means\n  cdef double a_mean\n  cdef double b_mean\n  a_mean = a_sum / a_len\n  b_mean = b_sum / b_len\n  \n  # We then put the difference to the means in new arrays\n  diff_a_samples = &lt;double *&gt;malloc(a_len*cython.sizeof(double))\n  if diff_a_samples is NULL:\n    raise MemoryError\n  diff_b_samples = &lt;double *&gt;malloc(b_len*cython.sizeof(double))\n  if diff_b_samples is NULL: \n    raise MemoryError\n\n  for i in range(a_len):\n    diff_a_samples[i] = a_samples_array[i] - a_mean\n    diff_b_samples[i] = b_samples_array[i] - b_mean\n\n  # This then allows us to easily compute the \n  # covariance and variances.  \n  cdef double covariance = 0\n  for i in range(a_len):\n    covariance += diff_a_samples[i] * diff_b_samples[i]\n\n  cdef double variance_a = 0\n  cdef double variance_b = 0\n  for i in range(a_len):\n    variance_a += diff_a_samples[i] ** 2\n    variance_b += diff_b_samples[i] ** 2\n\n\n  cdef double correlation = 0\n  cdef double variance_product = (variance_a * variance_b)\n  correlation = covariance / sqrt(variance_product)\n\n  free(a_samples_array)\n  free(b_samples_array)\n\n  return correlation\n\n\n%timeit cython_correlation(a,b)\n\n# 10000 loops, best of 5: 154 ¬µs per loop\n\nNice! We got a 6X improvement compared to Numpy and 15X improvement compared to pure Python. Pretty cool."
  },
  {
    "objectID": "riddles.html",
    "href": "riddles.html",
    "title": "Rcambier‚Äôs Blog",
    "section": "",
    "text": "Three‚Äôs a Crowd\n\n\n\n\n\n\n\n\n\n\n\nJan 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nThe newcomb poison\n\n\n\n\n\n\n\n\n\n\n\nJan 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nThe blind pill\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nReversed number\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nThe rope triangle\n\n\n\n\n\n\n\n\n\n\n\nApr 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nPotatoes\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nSurprise birthday\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Maths\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n4 Pieces\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nKing‚Äôs poison\n\n\n\n\n\n\n\n\n\n\n\nDec 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nPrisoner‚Äôs switch\n\n\n\n\n\n\n\n\n\n\n\nDec 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nLost father\n\n\n\n\n\n\n\n\n\n\n\nDec 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nGold coins\n\n\n\n\n\n\n\n\n\n\n\nDec 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nCats and mouses\n\n\n\n\n\n\n\n\n\n\n\nDec 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nRunning in circle\n\n\n\n\n\n\n\n\n\n\n\nDec 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nBroken plane\n\n\n\n\n\n\n\n\n\n\n\nDec 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nEvil professor\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nLost in the woods\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nPeeking\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nGender\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nPrisoners and trees\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nBlue eyes\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nException\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nMagic logic\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nLighter\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-01-14-shap-values.html",
    "href": "posts/2023-01-14-shap-values.html",
    "title": "SHAP values from scratch",
    "section": "",
    "text": "SHAP values are great for understanding a model. The basic logic of how they work is quite simple and worth trying to reproduce with raw Python.\nOn a high level, the logic is the following: To get the importance of a feature, let‚Äôs remove then add it back, on every sample. By checking the impact of removing/adding the feature on the prediction for that sample, we have the importance of the feature for that sample. We can then aggregate to get the feature importance overall. This is why we loop over all samples and all features in each sample below.\nThe only thing to add is that if we just use the existing samples, we only compute the impact of removing/adding the feature when all other features of the sample are present. The point of SHAP is to perform the operation with every combination of other features removed. This is why below we compute the coalitions.\nimport pandas as pd\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sympy.utilities.iterables import multiset_permutations\nfrom tqdm.notebook import tqdm\nfrom pprint import pprint\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport random"
  },
  {
    "objectID": "posts/2023-01-14-shap-values.html#creating-coalitions",
    "href": "posts/2023-01-14-shap-values.html#creating-coalitions",
    "title": "SHAP values from scratch",
    "section": "2.1 Creating ‚Äúcoalitions‚Äù",
    "text": "2.1 Creating ‚Äúcoalitions‚Äù\nCoalitions are list of 0s and 1s representing all possible combinations of features.\nIdeally, we would use all possible coalitions, to cover all the possible cases. In practice, extreme coalitions (mostly 1s and mostly 0s) are used. In our case, as we use pure Python, we will only consider a few coalitions.\nThis image, from the great SHAP explanation here, shows the way coalitions are used to remove features and replace them by the average values (in red)\n\n\ncoalitions = []\nfor i in [0,1, len(X.columns)-1, len(X.columns)]: \n  num_ones = i \n  num_zeroes = len(X.columns) - i\n  coalitions += list(multiset_permutations([0] * num_zeroes + [1] * num_ones))\n\npd.DataFrame(coalitions).style.applymap(lambda v: 'background-color:red' if v == 1 else None)\n\n\n\n\n\n\n¬†\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n5\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n6\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n7\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n8\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n9\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n10\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n11\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n12\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n13\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n14\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n15\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n16\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n17\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n18\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n19\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n20\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n21\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n22\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n23\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n24\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n25\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n26\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n27\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n28\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n29\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n30\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n31\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n32\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n33\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n34\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n35\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n36\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n37\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n38\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n39\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n40\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n41\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n42\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n43\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n44\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n45\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n46\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n47\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n48\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n49\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n50\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n51\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n52\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n53\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n1\n\n\n54\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n1\n\n\n55\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n1\n\n\n56\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n1\n\n\n57\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n1\n\n\n58\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n1\n\n\n59\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n1\n\n\n60\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n\n\n61\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1"
  },
  {
    "objectID": "posts/2023-01-14-shap-values.html#computing-the-feature-impact-simple-approach",
    "href": "posts/2023-01-14-shap-values.html#computing-the-feature-impact-simple-approach",
    "title": "SHAP values from scratch",
    "section": "2.2 Computing the feature impact (simple approach)",
    "text": "2.2 Computing the feature impact (simple approach)\nHere is the pseudo-logic - For every coalition - For every sample in the dataset, replace ‚Äú0‚Äù in coalition with average feature value for that sample - For every feature of the sample - Compute model score wihtout and with the feature. The difference is the impact of that feature, on that sample, for that coalition.\nAggregate all the impacts\n\n# We will gather all feature impacts \nfeature_impacts = {col:[] for col in X.columns}\n\n# The mean values are used for when a feature needs to be \"removed\"\nmean_values = X.mean().values\n\n# For each coalition we have prepared (1s and 0s)\nfor coalition in tqdm(pd.DataFrame(coalitions).astype(bool).values, total=len(coalitions)): \n\n  # For each sample in the dataset\n  for sample in X.values: \n\n    # In the sample, replace all the '0s' of the coalition\n    # with the mean of that feature\n    sample_masked = sample.copy()\n    sample_masked[~coalition] = mean_values[~coalition] \n\n    # For each feature\n    for feature_idx, feature_name in enumerate(X.columns):\n\n      # Compute the score without the feature\n      sample_without_feature = sample_masked.copy()\n      sample_without_feature[feature_idx] = mean_values[feature_idx]\n      score_without_feature = lr.predict_proba([sample_without_feature])[0][1]\n\n      # Compute the score with the feature\n      sample_with_feature = sample_masked.copy()\n      sample_with_feature[feature_idx] = sample[feature_idx]\n      score_with_feature = lr.predict_proba([sample_with_feature])[0][1]\n\n      # Store the feature impact\n      feature_impacts[feature_name].append(score_with_feature - score_without_feature)\n\n\n\n\n\npd.DataFrame(feature_impacts, columns=X.columns).head()\n\n\n  \n    \n      \n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n0\n0.722058\n-0.126389\n0.869852\n-0.128361\n-0.000087\n-0.003189\n-0.005405\n-0.001073\n-3.318434e-04\n-0.000017\n...\n0.870383\n0.780434\n-0.128907\n-0.128907\n-0.000214\n-0.022046\n-0.028566\n-0.003150\n-0.002920\n-0.000195\n\n\n1\n0.856055\n-0.060093\n0.871019\n-0.128904\n0.000046\n0.000479\n0.000049\n-0.000233\n-2.085911e-07\n0.000007\n...\n0.870042\n0.188555\n-0.128906\n-0.128907\n0.000061\n0.003977\n0.002225\n-0.001500\n0.000262\n-0.000028\n\n\n2\n0.837193\n0.137117\n0.870927\n-0.128886\n-0.000052\n-0.001030\n-0.002803\n-0.000864\n-1.407096e-04\n0.000003\n...\n0.866727\n0.008583\n-0.128905\n-0.128905\n-0.000086\n-0.009560\n-0.012331\n-0.002686\n-0.001230\n-0.000020\n\n\n3\n-0.117599\n0.066944\n-0.126242\n0.790957\n-0.000182\n-0.003304\n-0.003923\n-0.000616\n-4.289558e-04\n-0.000038\n...\n-0.092544\n-0.039935\n0.412002\n0.789438\n-0.000554\n-0.031548\n-0.027105\n-0.002987\n-0.006344\n-0.000496\n\n\n4\n0.851585\n-0.113725\n0.871053\n-0.128902\n-0.000016\n-0.000529\n-0.002818\n-0.000606\n1.432269e-06\n0.000004\n...\n0.858891\n0.804422\n-0.128905\n-0.128897\n-0.000036\n0.002886\n-0.008952\n-0.001008\n0.000934\n0.000040\n\n\n\n\n5 rows √ó 30 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\npd.DataFrame(feature_impacts).abs().mean().sort_values().iloc[-10:].plot.barh(figsize=(10, 6))"
  },
  {
    "objectID": "posts/2018-12-26-potatoes.html",
    "href": "posts/2018-12-26-potatoes.html",
    "title": "Potatoes",
    "section": "",
    "text": "A farmer has 100kg of potatoes. At the start, they are composed of 99% of water (the water is 99% of the total weight) and 1% of dry matter (the dry matter is 1% of the total weight). Later, during storage and because of evaporation, the water percentage drops to 98%. What is the total weight of the potatoes then?\n\nHover to show the answer.\n\n\n\n50kg.\nAt the start, there are 99kg of water and 1kg of dry matter. Later, the quantity of dry matter didn‚Äôt change, but it is now representing 2% of the total mass. If 1kg is 2%, then 100% is 50kg."
  },
  {
    "objectID": "posts/2018-12-10-peeking.html",
    "href": "posts/2018-12-10-peeking.html",
    "title": "Peeking",
    "section": "",
    "text": "Jack is looking at Anne, but Anne is looking at George. Jack is married, but George is not. Is a married person looking at an unmarried person?\n\nA: Yes\nB: No\nC: Cannot be determined\n\n\nHover to show the answer.\n\n\n\nYes.\nThere are only two possible cases: either Anne is married, or she is not.\nIf she is married, then a married person (Jack) is looking at an unmarried person (Anne). If she is not married, then a married person (Anne) is looking at an unmarried person (George)."
  },
  {
    "objectID": "posts/2018-12-25-prisoners-switch.html",
    "href": "posts/2018-12-25-prisoners-switch.html",
    "title": "Prisoner‚Äôs switch",
    "section": "",
    "text": "The warden meets with 23 new prisoners when they arrive. He tells them, ‚ÄúYou may meet today and plan a strategy. But after today, you will be in isolated cells and will have no communication with one another.\n‚ÄúIn the prison there is a switch room which contains two light switches labeled A and B, each of which can be in either the ‚Äòon‚Äô or the ‚Äòoff‚Äô position. BOTH SWITCHES ARE IN THEIR OFF POSITIONS NOW.* The switches are not connected to anything.\n‚ÄúAfter today, from time to time whenever I feel so inclined, I will select one prisoner at random and escort him to the switch room. This prisoner will select one of the two switches and reverse its position. He must move one, but only one of the switches. He can‚Äôt move both but he can‚Äôt move none either. Then he‚Äôll be led back to his cell.‚Äù\n‚ÄúNo one else will enter the switch room until I lead the next prisoner there, and he‚Äôll be instructed to do the same thing. I‚Äôm going to choose prisoners at random. I may choose the same guy three times in a row, or I may jump around and come back.‚Äù\n‚ÄúBut, given enough time, everyone will eventually visit the switch room as many times as everyone else. At any time any one of you may declare to me, ‚ÄòWe have all visited the switch room.‚Äô\n‚ÄúIf it is true, then you will all be set free. If it is false, and somebody has not yet visited the switch room, you will be fed to the alligators.‚Äù\n*note - the only difference from Scenario B, the original position of the 2 switches are known.\n\nHover to show the answer.\n\n\n\nThe strategy is the following:\nThey choose one prisoner that will be the counter. Let‚Äôs name the two switches A and B. The counter is the only prisoner allowed to turn switch A off. When the other prisoner enter the room, if they have never done it, and if the switch A is off, they put it to ON. If they already put the A switch to ON or if they enter en the A switch is ON, they simply switch the B switch, in whatever position.\nThe counter will be able to count the number of prisoner that have entered the room by counting the number of times he has to put the A switch to OFF."
  },
  {
    "objectID": "posts/2018-12-25-lost-father.html",
    "href": "posts/2018-12-25-lost-father.html",
    "title": "Lost father",
    "section": "",
    "text": "Question: A mother is 21 years older than her child. In 6 years the mother will be 5 times older than her baby.\nWhere is the father?\n\nHover to show the answer.\n\n\n\n\nM = mother\nC = child\nM = C + 21\nM + 6 = 5* (C + 6)\nSolving gives C = -3/4\n-3/4 year means minus 9 months. If the child is -9 months, it means that the father is with the mother, in their bed‚Ä¶"
  },
  {
    "objectID": "posts/2021-09-20-random-forest.html",
    "href": "posts/2021-09-20-random-forest.html",
    "title": "Random forest",
    "section": "",
    "text": "from sklearn.datasets import load_breast_cancer\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import f1_score\nimport random\n\n\nraw = load_breast_cancer(return_X_y=True)\n\nX = pd.DataFrame(raw[0])\ny = pd.DataFrame(raw[1])\n\n\nfeatures = X.columns\nn_features = len(features)\nn_features_to_consider = int(round(np.sqrt(n_features)))\nfeatures, n_features_to_consider\n\n(RangeIndex(start=0, stop=30, step=1), 5)\n\n\n\ntrees = []\n\nfor i in range(10): \n    feature_subset = random.sample(features.values.tolist(), k=n_features_to_consider)\n    tree = DecisionTreeClassifier(max_depth=2)\n    sampling_index = X.sample(frac=1).index # RANDOMly select data to train on \n    tree.fit(X.loc[sampling_index, feature_subset], y.loc[sampling_index]) # RANDOMly select features to train on\n    trees.append((tree, feature_subset))\n\n\nrf_predictions = np.mean([tree.predict(X.loc[:, features]) for tree, features in trees], axis=0)\n\n\n# The precision of a single tree\nfor i in range(5):\n    one_tree_predictions = trees[i][0].predict(X.loc[:, trees[i][1]])\n    print(f1_score(y, one_tree_predictions) )\n\n0.9195088676671215\n0.9346879535558781\n0.9439124487004104\n0.9482517482517482\n0.9410187667560322\n\n\n\n# The precision of the forest\nf1_score(y, (rf_predictions &gt; 0.5) * 1)\n\n0.9665738161559889"
  },
  {
    "objectID": "posts/2018-12-25-gold-coins.html",
    "href": "posts/2018-12-25-gold-coins.html",
    "title": "Gold coins",
    "section": "",
    "text": "There is a stack of 100 coins. Each coin has a silver side and a golden side. 20 coins are silver side up. The rest is golden side up. You are in a totally dark room. How do you make 2 stacks that contain as many coins with silver side up ? (we don‚Äôt care about how many are golden side up in each group) There is no way to differentiate the coins in the dark (by touch or other means.)\n\nHover to show the answer.\n\n\n\nYou take 20 coins from the stack and flip them to create another stack. The two stacks are the original stack withtout the 20 coins, and the stack of 20 coins you created.\nIf the 20 coins you select are golden side up, then you will flip them and have 20 silver coins in each stack.\nIf the 20 coins you select are silver side up, then you will flip them and have 0 silver coins in each stack.\nIf the 20 coins you select contain n silver and 20-n gold coins, then you will flip them and have 20-n silver coins in each stack."
  },
  {
    "objectID": "posts/2021-09-14-linear-regression.html",
    "href": "posts/2021-09-14-linear-regression.html",
    "title": "Linear regression & Logistic regression",
    "section": "",
    "text": "import scipy as sp \nimport numpy as np \nimport pandas as pd\nfrom sklearn.metrics import r2_score, precision_score, recall_score, log_loss\nfrom sklearn.linear_model import LinearRegression, Ridge\nimport sklearn\n\n\nSome data\n\ndf = pd.read_csv(\"https://rcambier.github.io/blog/assets/california_housing_train.csv\")\ndf = df[['housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value']]\n\n\n\nLinear Regression\n\nscaled_df = (df - df.min()) / (df.max() - df.min())\nX = scaled_df[['housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']].values\ny = scaled_df['median_house_value'].values\n\nX_with_intercept = np.hstack((np.ones((len(X), 1)),X))\nB = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ (X_with_intercept.T @ y.reshape(-1, 1))\n\nprint(\"Manual weights: \", B.reshape(-1))\nprint(\"Manual score: \", r2_score(y, (X_with_intercept @ B).reshape(-1)))\n\nManual weights:  [-0.07556544  0.19769139 -1.56087573  1.32234017 -2.57610401  1.59516284\n  1.43606576]\nManual score:  0.5713482748283873\n\n\n\nfrom sklearn.metrics import r2_score\n\nRSS = (((X_with_intercept @ B).reshape(-1) - y)**2).sum() # Squared distance from our new regression line\nTSS = ((y.mean() - y)**2).sum()                           # Squared distance from the mean\nr2 = 1 - RSS / TSS                                        # How much distance did we gained ? Did we reduce the errors ? Are we closer to the actual point values ?\nr2, r2_score(y, (X_with_intercept @ B).reshape(-1))\n\n(0.5713482748283873, 0.5713482748283873)\n\n\nLet‚Äôs compare those results with sklearn linear regression\n\n\nlr = LinearRegression().fit(X, y)\n\nprint(\"\")\nprint(\"Sklearn weights: \", [lr.intercept_] + lr.coef_.tolist() )\nprint(\"Sklearn score: \", r2_score(y, lr.predict(X)))\n\n\nSklearn weights:  [-0.07556543642855085, 0.19769138728528463, -1.560875734209466, 1.3223401715433833, -2.5761040065353327, 1.5951628411047127, 1.4360657609756604]\nSklearn score:  0.5713482748283873\n\n\n\n\nLinear regression with regularization (Ridge regression)\nRegularization is the action of adding to the loss, a term that contains the weight values. That way these terms are forced to stay small. This helps avoiding overfitting.\nLet‚Äôs look at the ordinary least sqaure loss and then add the square of each weight to build the regularized loss. Adding the square of each weight means we buil the Ridge regression loss. If we add the absolute value of each weight we build the Lasso regression loss.\n\n\nloss = (((X_with_intercept @ B) - y.reshape(-1, 1)).T  @  (X_with_intercept @ B) - y.reshape(-1, 1)).reshape(-1)\nregularized_loss = loss + 0.3 * B.T @ B\nloss, regularized_loss\n\n(array([-4.07656752, -4.10378391, -4.11533025, ..., -4.15223732,\n        -4.11553644, -4.13368069]),\n array([[-0.1053435 , -0.13255988, -0.14410623, ..., -0.18101329,\n         -0.14431241, -0.16245667]]))\n\n\nThe way adding this loss impacts the formula is the following\n\nscaled_df = (df - df.min()) / (df.max() - df.min())\nX = scaled_df[['housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']].values\ny = scaled_df['median_house_value'].values\n\n\nX_with_intercept = np.hstack((np.ones((len(X), 1)),X))\n\nI = np.identity(X_with_intercept.shape[1])\nI[0,0] = 0\nB = np.linalg.inv(X_with_intercept.T @ X_with_intercept + 0.3 * I) @ (X_with_intercept.T @ y.reshape(-1, 1))\n\nprint(\"Manual weights: \", B.reshape(-1))\nprint(\"Manual score: \", r2_score(y, (X_with_intercept @ B).reshape(-1)))\n\nManual weights:  [-0.07457501  0.19926227 -1.4614579   1.30386275 -2.31228351  1.40463349\n  1.42708759]\nManual score:  0.5710213053584052\n\n\n\n\nlr = Ridge(alpha=0.3).fit(X, y)\n\nprint(\"\")\nprint(\"Sklearn weights: \", [lr.intercept_] + lr.coef_.tolist() )\nprint(\"Sklearn score: \", r2_score(y, lr.predict(X)))\n\n\nSklearn weights:  [-0.07457500943073836, 0.19926227134208827, -1.4614578956147966, 1.3038627486538301, -2.3122835137561593, 1.4046334910837222, 1.4270875901070914]\nSklearn score:  0.5710213053584055\n\n\n\n\nLogistic Regression\nFor the logistic regression, we transform the X values in the same way but we add a sigmoid transform at the end in order to map to values between 0 and 1.\nWe can not use the normal form anymore for computing the weights. We have to resort to other techniques like gradient descent.\n\ndef sigmoid(x):\n  return  1 / (1 + np.exp(-x)) \n\ndef log_likelihood(y_hat, y_true):\n  # Being far away from the correct class is penalized heavily. \n  return - np.mean( y_true * np.log(y_hat) + (1-y_true) * np.log(1-y_hat) )\n\ndef gradient_sigmoid(x):\n  sigmoid(X) * (1 - sigmoid(X))\n\n\ndef gradients(X, y, y_hat):\n    # Loss = y * log(h) + (1 - y) * log(1-h)\n    # where h = sigmoid(z)\n    # and z = Xt @ B\n\n    # deriv_loss_to_h = y / h - (1-y) / (1-h) = (y - h) / (h * (1 - h))\n    # deriv_h_to_z = sigmoid(h) * (1 - sigmoid(h))\n    # deriv_z_to_b = Xt\n    # Though chain rule, final derivative \n    # final_derivative = deriv_loss_to_h * deriv_h_to_z * deriv_z_to_b = x * (y - h) = x * (y - y_hat) \n    dw = (1/len(X)) * (X.T @ (y_hat - y))\n    return dw\n\n\ndf['median_house_value_cat'] = (df['median_house_value'] &gt; 150_000).astype(int)\n\nscaled_df = (df - df.min()) / (df.max() - df.min())\nX = scaled_df[['housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']].values\ny = df['median_house_value_cat'].values \n\nX_with_intercept = np.hstack((np.ones((len(X), 1)),X))\n\nB = np.random.normal(0, 0.1 ,(7, 1))\n\nfor i in range(50_000):\n  y_hat = sigmoid(X_with_intercept @ B).reshape(-1)\n  if i % 5000 == 0 or i ==0: \n    print(\"loss: \", log_likelihood(y_hat, y))\n  deltas = gradients(X_with_intercept, y, y_hat)\n  B -= 0.3 * deltas.reshape(-1, 1)\n\n\nlr = sklearn.linear_model.LogisticRegression().fit(X, y)\n\nloss:  0.6821755251690551\nloss:  0.46252854158211454\nloss:  0.4541283502467595\nloss:  0.4510517001241438\nloss:  0.4487185438489886\nloss:  0.4466409644195144\nloss:  0.44474208933519344\nloss:  0.4429996946665173\nloss:  0.4413999204051543\nloss:  0.43993073757337586\n\n\n\nprint(\"Manual weights: \", B.reshape(-1))\nprint(\"Manual score: \", \n        precision_score(y, (sigmoid(X_with_intercept @ B).reshape(-1) &gt; 0.5).astype(int) ),\n        recall_score(y, (sigmoid(X_with_intercept @ B).reshape(-1) &gt; 0.5).astype(int) ),\n      )\nprint()\nprint(\"Sklearn log loss: \", log_loss(y, (sigmoid(X_with_intercept @ B).reshape(-1))))\nprint(\"Sklearn weights: \", lr.intercept_.tolist() + lr.coef_.reshape(-1).tolist())\nprint(\"Sklearn score\", \n      precision_score(y, lr.predict(X)),\n      recall_score(y, lr.predict(X))\n      )\n\nManual weights:  [ -4.74650191   2.09565859 -11.5802056    7.08212211  -3.00163538\n   8.00035044  18.85028779]\nManual score:  0.8215249055925193 0.8514583915758084\n\nSklearn log loss:  0.43866521703018374\nSklearn weights:  [-4.385273936252051, 1.9603353158729624, -10.78106293024599, 6.882196980429938, -2.868679885031378, 7.251350300146187, 17.41000987846787]\nSklearn score 0.8186773905272565 0.8536949026185817\n\n\nThe weights are not exactly the same but the performances are very similar. This is due to the randomness aspect of training through gradient descent."
  },
  {
    "objectID": "posts/2018-12-18-evil-professor.html",
    "href": "posts/2018-12-18-evil-professor.html",
    "title": "Evil professor",
    "section": "",
    "text": "The evil professor says to the students: you are going to have an exam next week. I‚Äôm not telling you which day, but I am telling you that it will be unexpected (i.e., the day of the exam you won‚Äôt be sure whether the exam is that day or not). I‚Äôm assuming that the week starts on Monday and ends on Friday.\nSo this is what the students think:\nOkay, it can‚Äôt be on Friday, because if Friday comes and we haven‚Äôt had the test yet, then in the morning we‚Äôll surely know it‚Äôs that day. So it will be a day from Monday to Thursday. After pondering a bit, they realise that it can‚Äôt be Thursday either, for the same reason: If Thursday comes and they haven‚Äôt had the exam they‚Äôll think ‚Äúwe know it‚Äôs not Friday, therefore it has to be today‚Äù. So Thursday wouldn‚Äôt be a surprise either. Similarly (by induction), all days are eliminated !\nThen they are happy and think the evil professor made a promise he couldn‚Äôt hold, and don‚Äôt prepare for the test. However, the next week, on Wednesday, they have the exam. And all are surprised.\nWhere is the flaw in the logic ?\n\nHover to show the answer.\n\n\n\nThere is no answer to this‚Ä¶ I will try to explain later."
  },
  {
    "objectID": "posts/2018-12-10-blue-eyes.html",
    "href": "posts/2018-12-10-blue-eyes.html",
    "title": "Blue eyes",
    "section": "",
    "text": "A group of people with assorted eye colors live on an island. They are all perfect logicians ‚Äì if a conclusion can be logically deduced, they will do it instantly. No one knows the color of their eyes. Every night at midnight, a ferry stops at the island. Any islanders who have figured out the color of their own eyes then leave the island, and the rest stay. Everyone can see everyone else at all times and keeps a count of the number of people they see with each eye color (excluding themselves), but they cannot otherwise communicate. Everyone on the island knows all the rules in this paragraph.\nOn this island there are 100 blue-eyed people, 100 brown-eyed people, and the Guru (she happens to have green eyes). So any given blue-eyed person can see 100 people with brown eyes and 99 people with blue eyes (and one with green), but that does not tell him his own eye color; as far as he knows the totals could be 101 brown and 99 blue. Or 100 brown, 99 blue, and he could have red eyes.\nThe Guru is allowed to speak once (let‚Äôs say at noon), on one day in all their endless years on the island. Standing before the islanders, she says the following:\n‚ÄúI can see someone who has blue eyes.‚Äù\nWho leaves the island, and on what night?\nThere are no mirrors or reflecting surfaces, nothing dumb. It is not a trick question, and the answer is logical. It doesn‚Äôt depend on tricky wording or anyone lying or guessing, and it doesn‚Äôt involve people doing something silly like creating a sign language or doing genetics. The Guru is not making eye contact with anyone in particular; she‚Äôs simply saying ‚ÄúI count at least one blue-eyed person on this island who isn‚Äôt me.‚Äù\nAnd lastly, the answer is not ‚Äúno one leaves.‚Äù\n\nHover to show the answer.\n\n\n\nAll the 100 blue-eye people leave the island on the 100th night.\nThe following chart explains why using 4 people. The reflexion is the same, simply longer, for 100 persons. Click on the chart to open it in a new window.\n\n\n\n\n\n\nThe information ‚ÄúThere is at least one blue-eyed person‚Äù is needed !\n\n\nThe information ‚ÄúThere is at least one blue-eyed person‚Äù is needed !\n\n\n\n\n\n\nIf I am blue-eyed, he looks at 3 blue-eyed persons\n\n\nIf I am blue-eyed, he looks at 3 blue-eyed persons\n\n\n\n\n\n\nIf I am brown eyed, he looks at only 2 other blue-eyed\n\n\nIf I am brown eyed, he looks at only 2 other blue-eyed\n\n\n\n\n\n\nWhen I look at 3 blue-eyed person, I wonder if I am myself blue eyed.\n\n\nWhen I look at 3 blue-eyed person, I wonder if I am myself blue eyed.\n\n\n\n\n\n\nI consider myself blue-eyed\n\n\nI consider myself blue-eyed\n\n\n\n\n\n\nI consider myself brown-eyed\n\n\nI consider myself brown-eyed\n\n\n\n\n\n\nME\n\n\nME\n\n\n\n\n\n\nI wonder if this guy thinks he is blue eyed or not (I wonder the same for the 2 other, the reflexion is the same)\n\n\n[Not supported by viewer]\n\n\n\n\n\n\nME\n\n\nME\n\n\n\n\n\n\nA\n\n\nA\n\n\n\n\n\n\nB\n\n\nB\n\n\n\n\n\n\nC\n\n\nC\n\n\n\n\n\n\nHe considers himself blue-eyed\n\n\nHe considers himself blue-eyed\n\n\n\n\n\n\nHe considers himself brown-eyed\n\n\nHe considers himself brown-eyed\n\n\n\n\n\n\nME\n\n\nME\n\n\n\n\n\n\nA\n\n\nA\n\n\n\n\n\n\nB\n\n\nB\n\n\n\n\n\n\nC\n\n\nC\n\n\n\n\n\n\nIf A considers himself blue-eyed, there are 3 blue-eyed persons.\n\n\nIf A considers himself blue-eyed, there are 3 blue-eyed persons.\n\n\n\n\n\n\nME\n\n\nME\n\n\n\n\n\n\nA\n\n\n[Not supported by viewer]\n\n\n\n\n\n\nB\n\n\nB\n\n\n\n\n\n\nC\n\n\nC\n\n\n\n\n\n\nIf A doesn‚Äôt consider himself blue-eyed, he will try to understand what the 2 last one are going to do¬†\n\n\n[Not supported by viewer]\n\n\n\n\n\n\nThis last guy could think he is brown-eyed\n\n\nThis last guy could think he is brown-eyed\n\n\n\n\n\n\nThis last guy could think he is blue-eyed\n\n\nThis last guy could think he is blue-eyed\n\n\n\n\n\n\nME\n\n\nME\n\n\n\n\n\n\nA\n\n\nA\n\n\n\n\n\n\nB\n\n\nB\n\n\n\n\n\n\nC\n\n\nC\n\n\n\n\n\n\nThen there are 2 blue-eyed persons¬†\n\n\nThen there are 2 blue-eyed persons¬†\n\n\n\n\n\n\nME\n\n\nME\n\n\n\n\n\n\nA\n\n\nA\n\n\n\n\n\n\nB\n\n\nB\n\n\n\n\n\n\nC\n\n\nC\n\n\n\n\n\n\nThen he looks at a blue-eye guy that is alone\n\n\nThen he looks at a blue-eye guy that is alone\n\n\n\n\n\n\nME\n\n\nME\n\n\n\n\n\n\nA\n\n\nA\n\n\n\n\n\n\nB\n\n\nB\n\n\n\n\n\n\nC\n\n\nC\n\n\n\n\n\n\nIf someone comes and say ‚ÄúThere is at least one blue-eyed person‚Äù, than person B will see 3 brown-eyed and leave !\n\n\n[Not supported by viewer]\n\n\n\n\n\n\nIf someone comes and say ‚ÄúThere is at least one blue-eyed person‚Äù, Than, no one will dare to leave the first day !\n\n\n[Not supported by viewer]\n\n\n\n\n\n\nAfter one day, we know which one of those 2 supposed situations is the true one.Either B left, and there was only 1 blue-eyed person.Or B stays, and C will understand that he is blue after one day.¬†\n\n\n[Not supported by viewer]\n\n\n\n\n\n\nHowever, if after two days, B and C are not leaving‚Ä¶A will understand that his supposition that he is brown might be false.The third day, A, B and C are going to leave\n\n\n[Not supported by viewer]\n\n\n\n\n\n\nIf the 3rd day no one leaves..¬†I should understand that the supposition that I am brown can not hold.¬†Therefore, we are all blue-eyed.The 4th day, we all leave.\n\n\n[Not supported by viewer]\n\n\n\n\n\n\nThe problem is the same with 4 persons and with 100 persons.This is the concept of common-knowledge.¬†At first, I might think that it is common knowledge that there are at least 3 blue-eyed person. And I might think that the sentence ‚Äúthere is at least one blue-eyed person‚Äù is useless, since everyone sees at least two blue eyed person.¬†But we don‚Äôt all have the same knowledge. If I am browm eyed, other people might only see 2 blue-eyed persons. Those persons will think that other could only see 1 blue-eyed person. And those persons that see only one blue-eyed person could think that someone sees no blue-eyed persons.¬†This means that, until someone says ‚ÄúI see at least one blue eyed person‚Äù and 4 day passes, we don‚Äôt all have the same knowledge about the world.The time it takes to reach common knowledge is the time the persons have to wait before leaving.¬†\n\n\n[Not supported by viewer]\n\n\n\n\n\n\nYes this is comic sans ms :)\n\n\nYes this is comic sans ms :)&lt;br&gt;"
  },
  {
    "objectID": "posts/2018-12-10-one-out.html",
    "href": "posts/2018-12-10-one-out.html",
    "title": "Exception",
    "section": "",
    "text": "Find the exception\n{:width=‚Äú400px‚Äù}\n\nHover to show the answer.\n\n\n\nB is the exception.\nIt is the only one that is only one difference away from every other one.\n\nA and B have radius in difference\nA and C have color in difference\nA and D have snowfloake in difference\nA and E have shape in diffrence\nA and F have border in difference.\n\nIf you select another one, like D, it can have 2 differences to some."
  },
  {
    "objectID": "posts/2022-04-01-multiprocessing.html",
    "href": "posts/2022-04-01-multiprocessing.html",
    "title": "Python multiprocessing with loading bar",
    "section": "",
    "text": "Multiprocessing in Python is already not the best. But on top of it, I always want to add a loading bar that tells me how much work has been performed. It took me a while to figure out how to best do that.\nWhat I want is: - Work gets done in parrallel, either in threads or in processes depending on how much GIL locking there is in my function. - The loading bar progresses as work gets done. - When the progress bars hits the end, work is finished. - You can pipe a generator into the parrallel processing, and it will be consumed progressively\nWhat I settled for is the below code. It consumes the iterable generator progressively, and displays a progress bar indicating how much work has been achieved."
  },
  {
    "objectID": "posts/2022-04-01-multiprocessing.html#why-not-use-concurrent.futures",
    "href": "posts/2022-04-01-multiprocessing.html#why-not-use-concurrent.futures",
    "title": "Python multiprocessing with loading bar",
    "section": "Why not use concurrent.futures ?",
    "text": "Why not use concurrent.futures ?\nBecause this option does not allow for the imap multiprocessing. This means that all the iterable will be consumed before being sent to the workers. This could be fine, but sometimes, if the iterable takes time to compute or is a generator itself, you don‚Äôt want to consume it fully before starting the concurrent processing.\nTry the code below. Notice that the loading bar starts appearing once the iterable has been consumed, which means it already reached the 13/20 iteration.\n\nfrom concurrent.futures import ProcessPoolExecutor\nfrom tqdm.auto import tqdm \nimport time\n\ndef work_function(arg): \n    time.sleep(arg)\n    return arg\n\ndef iterable():\n    for i in range(0,20):\n        time.sleep(i/10)\n        yield i\n\nwith ProcessPoolExecutor(10) as p: \n    results = list(tqdm(p.map(work_function, iterable(), chunksize=1)))\n    print(results)\n    print(\"done\")"
  },
  {
    "objectID": "posts/2018-12-26-surprise-birthday.html",
    "href": "posts/2018-12-26-surprise-birthday.html",
    "title": "Surprise birthday",
    "section": "",
    "text": "Albert and Bernard just become friends with Cheryl, and they want to know when her birthday is. Cheryl gives them a list of 10 possible dates.\n\nMay 15\nMay 16\nMay 19\nJune 17\nJune 18\nJuly 14\nJuly 16\nAugust 14\nAugust 15\nAugust 17\n\nChely then tells Albert and Bernard separately the month and the day of her birthday respectively.\nAlbert: I don‚Äôt know when Cheryl‚Äôs birthday is, but I know that Bernard does not know too.\nBernard: At first I don‚Äôt know when Cheryl‚Äôs birthday is, but I know now.\nAlbert: Then I also know when Cheryl‚Äôs birthday is.\nSo when is Cheryl‚Äôs birthday ?\n\nHover to show the answer.\n\n\n\nIt is the July 16\n\nAlbert (A) knows the month.\nBernard (B) knows the day.\nIf A knows the month and knows that B doesn‚Äôt know, it means that it is a month that only contains day that you can also find in other proposed months. Otherwise A wouldn‚Äôt be so sure that B doens‚Äôt know. So it cannot be May because if it was May 19, B would know. It cannot be June, because if it was June 18, B would know. It can be July or August.\nB says he now knows when the birthday is. B just deduced, like us, that it could be July or August because of what A just said. If he now knows when it is, it cannot be the 14. So it is the July 16, August 15 or August 17.\nA then says that he also now knows. So it has to be the July 16, since that is the only month where A has enough information to know the exact birthdate. If it was August, A would still need more information to differentiate between August 15 and August 17."
  },
  {
    "objectID": "posts/2022-04-29-background-jobs.html",
    "href": "posts/2022-04-29-background-jobs.html",
    "title": "Easy way to queue a job in the terminal",
    "section": "",
    "text": "There is a small trick I love to use when running long commands in my terminal.\nTo run a job in the background, you can append & to the end of it. But what if you just started running a command, and you now realize you wanted to queue another command after it? Since you forgot to add &, it‚Äôs now running in the foreground in your terminal and it‚Äôs not easy to queue another job after it. Well, here is how to do it:\n\nYou run your first command forgetting to add the & at the end\n\n$ for i in {1..5}; do sleep 1; done\n\nYou use CTRL + Z to pause that job and put it in the background. You should see\n\n&gt; [1]  + 93657 suspended  sleep 1\n\nYou use fg && [new command] in order to put that job back to the foreground and queue another job to it.\n\n$ fg && echo 'see!?'\n&gt; [1]  + 93657 continued  sleep 1\n&gt; see!?\n\nUsing the jobs feature\nThis is using the foreground/background job feature of the terminal. Here is a more detailed tutorial.\nTo summarize the few command I use: - CTRL + Z to put a job in the background - jobs to have a look at all the jobs - fg to put the last bakgrounded process back to the foreground - fg %2 to put another job back to the foreground - kill %2 to kill job number 2"
  },
  {
    "objectID": "posts/2021-09-15-pca.html",
    "href": "posts/2021-09-15-pca.html",
    "title": "PCA",
    "section": "",
    "text": "PCA\nThe principal components are the eigenvectors+eigenvalues of the Covariance matrix of our data.\nThis is because we are looking for the ‚ÄúDirection of stretching and how much streching happens‚Äù regarding the variance of our data.\n\nfrom sklearn.datasets import load_digits\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nimport pandas as pd \nimport numpy as np\n\ndigits = pd.DataFrame(load_digits()['data'])\nclasses = load_digits(return_X_y=True)[1]\n\n\nlow_dim_digits = PCA(n_components=2).fit_transform(digits)\nsns.scatterplot(x=low_dim_digits[:,0], y=low_dim_digits[:,1], hue=classes)\n\n\n\n\n\ndigits_normed = digits - digits.mean()\n\n# compute the covariance matrix \ncov_matrix = digits_normed.T  @ digits_normed / len(digits_normed) # same as digits_normed.cov()\neigen_values, eigen_vectors = np.linalg.eig(cov_matrix)\neigen_values, eigen_vectors\n\n# Sort eigen values end eigen vectors\nsorted_index = np.argsort(eigen_values)[::-1]\nsorted_eigenvalue = eigen_values[sorted_index]\nsorted_eigenvectors = eigen_vectors[:,sorted_index]\n\n# Select the 2 best\neigenvector_subset = sorted_eigenvectors[:, 0:2]\n\nX_reduced = np.dot(eigenvector_subset.transpose(), digits_normed.transpose()).transpose()\nsns.scatterplot(x=X_reduced[:,0], y=X_reduced[:,1], hue=classes)"
  },
  {
    "objectID": "posts/2018-12-25-running-in-circle.html",
    "href": "posts/2018-12-25-running-in-circle.html",
    "title": "Running in circle",
    "section": "",
    "text": "If I can make it around a running track once by going 10 km/h, what speed should I be running a second lap to have an average speed of 20 km/h for the two laps ?\n\nHover to show the answer.\n\n\n\nThis is impossible, I need to be infinitely fast.\nImagine a 10km track. It will take me 1 hour to make a lap at 10km/h. With a second lap, it becomes 20km. To have a 20km/h average speed, I need to do those 2 laps in 1 hour.\nBut I already spent 1 hour, so I should be infinitely fast for the second lap, which is impossible."
  },
  {
    "objectID": "posts/2018-12-18-lost-in-the-woods.html",
    "href": "posts/2018-12-18-lost-in-the-woods.html",
    "title": "Lost in the woods",
    "section": "",
    "text": "You are in a square forest of side 100km. You are 2km away from its border, but you don‚Äôt know in what direction or at what angle (It could be at 2km with an angle of 16.123 degree for example.) You need to exit the forest, but you can walk a maximum of 13km. What path do you follow ?\n\nHover to show the answer.\n\n\n\nClick the link to view an example of a path shorter than 13 km."
  },
  {
    "objectID": "posts/2020-07-29-pills.html",
    "href": "posts/2020-07-29-pills.html",
    "title": "The blind pill",
    "section": "",
    "text": "You are blind. You have 2 blue pills and 2 red pills in a box.\nYou have to take 2 pills today and 2 tomorrow. Each day, exactly one blue and one red. If you don‚Äôt, you will die. If you take more, you will die.\nHow do you do it ?\n\nHover to show the answer.\n\n\n\nOne by one, you take the pill, break it in half, eat a half and keep the other half for tomorrow. This way, you will eat exactly 1 blue and 1 red pill per day."
  },
  {
    "objectID": "posts/2020-04-19-rope-triangle.html",
    "href": "posts/2020-04-19-rope-triangle.html",
    "title": "The rope triangle",
    "section": "",
    "text": "You cut a rope in a random place. Then again in another random place.\nYou are left with 3 pieces.\nWhat are the chances that you can form a triangle with those 3 pieces ?\n\nHover to show the answer.\n\n\n\n1/4."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rcambier‚Äôs Blog",
    "section": "",
    "text": "Bayesian Inference from scratch in Python\n\n\n\n\n\n\nai\n\n\n\nBayesian inference, from scratch in simply Python without any prior knowledge\n\n\n\n\n\nOct 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSHAP values from scratch\n\n\n\n\n\n\nai\n\n\n\nHow SHAP values are computed, in a few simple Python lines\n\n\n\n\n\nJan 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEasy way to queue a job in the terminal\n\n\n\n\n\n\nprogramming\n\n\nsystem\n\n\n\nHow to use background jobs to quickly queue a job after a running command\n\n\n\n\n\nApr 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPython multiprocessing with loading bar\n\n\n\n\n\n\nprogramming\n\n\n\nCreate a nice multiprocessing logic with a loading bar\n\n\n\n\n\nApr 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nCython for fast python\n\n\n\n\n\n\nprogramming\n\n\n\nTrying out cython\n\n\n\n\n\nOct 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian riddle\n\n\n\n\n\n\nprogramming\n\n\nai\n\n\n\nThe typical bayesian interview question, solved with PyMC3\n\n\n\n\n\nSep 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nRandom forest\n\n\n\n\n\n\nai\n\n\n\nCreate random forest from scratch in Python\n\n\n\n\n\nSep 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nWord embedding\n\n\n\n\n\n\nai\n\n\n\nCreate word embeddings simply from scratch\n\n\n\n\n\nSep 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Boosting trees\n\n\n\n\n\n\nai\n\n\n\nSimple boosting trees, for regression and classification in python from scratch\n\n\n\n\n\nSep 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nSVD\n\n\n\n\n\n\nai\n\n\n\nSingular value decomposition from scratch\n\n\n\n\n\nSep 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nPCA\n\n\n\n\n\n\nai\n\n\n\nPCA dimensionality reduction from scratch\n\n\n\n\n\nSep 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nLinear regression & Logistic regression\n\n\n\n\n\n\nai\n\n\n\nLinear regression, ridge regression, logistic regression with r2 score from scratch in Python\n\n\n\n\n\nSep 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis testing\n\n\n\n\n\n\nai\n\n\n\nHypothesis testing, confidence interval\n\n\n\n\n\nSep 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nThree‚Äôs a Crowd\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nJan 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nThe newcomb poison\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nJan 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nThe blind pill\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nJul 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nReversed number\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nApr 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nThe rope triangle\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nApr 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nPotatoes\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 26, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nSurprise birthday\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 26, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Maths\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 26, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n4 Pieces\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 26, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nKing‚Äôs poison\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nPrisoner‚Äôs switch\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nLost father\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nGold coins\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nCats and mouses\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nRunning in circle\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nBroken plane\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nEvil professor\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 18, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nLost in the woods\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 18, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nPeeking\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nGender\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nPrisoners and trees\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nBlue eyes\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nException\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nMagic logic\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nLighter\n\n\n\n\n\n\nriddle\n\n\n\n\n\n\n\n\n\nDec 10, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "artificial_intelligence.html",
    "href": "artificial_intelligence.html",
    "title": "Rcambier‚Äôs Blog",
    "section": "",
    "text": "Bayesian Inference from scratch in Python\n\n\n\n\n\nBayesian inference, from scratch in simply Python without any prior knowledge\n\n\n\n\n\nOct 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSHAP values from scratch\n\n\n\n\n\nHow SHAP values are computed, in a few simple Python lines\n\n\n\n\n\nJan 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian riddle\n\n\n\n\n\nThe typical bayesian interview question, solved with PyMC3\n\n\n\n\n\nSep 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nRandom forest\n\n\n\n\n\nCreate random forest from scratch in Python\n\n\n\n\n\nSep 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nWord embedding\n\n\n\n\n\nCreate word embeddings simply from scratch\n\n\n\n\n\nSep 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Boosting trees\n\n\n\n\n\nSimple boosting trees, for regression and classification in python from scratch\n\n\n\n\n\nSep 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nSVD\n\n\n\n\n\nSingular value decomposition from scratch\n\n\n\n\n\nSep 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nPCA\n\n\n\n\n\nPCA dimensionality reduction from scratch\n\n\n\n\n\nSep 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nLinear regression & Logistic regression\n\n\n\n\n\nLinear regression, ridge regression, logistic regression with r2 score from scratch in Python\n\n\n\n\n\nSep 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis testing\n\n\n\n\n\nHypothesis testing, confidence interval\n\n\n\n\n\nSep 13, 2021\n\n\n\n\n\n\nNo matching items"
  }
]