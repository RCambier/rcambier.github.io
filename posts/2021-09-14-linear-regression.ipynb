{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /ai/2021/09/14/linear-regression\n",
    "badges: true\n",
    "categories:\n",
    "- ai\n",
    "date: '2021-09-14'\n",
    "description: Linear regression, ridge regression, logistic regression with r2 score\n",
    "  from scratch in Python\n",
    "output-file: 2021-09-14-linear-regression.html\n",
    "publishes: true\n",
    "title: Linear regression & Logistic regression\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, precision_score, recall_score, log_loss\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://rcambier.github.io/blog/assets/california_housing_train.csv\")\n",
    "df = df[['housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f1ZNuYtr0Bg"
   },
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZofxdDhmevD",
    "outputId": "258b2687-227d-4b00-cd43-6b0dc6a0eba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual weights:  [-0.07556544  0.19769139 -1.56087573  1.32234017 -2.57610401  1.59516284\n",
      "  1.43606576]\n",
      "Manual score:  0.5713482748283873\n"
     ]
    }
   ],
   "source": [
    "scaled_df = (df - df.min()) / (df.max() - df.min())\n",
    "X = scaled_df[['housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']].values\n",
    "y = scaled_df['median_house_value'].values\n",
    "\n",
    "X_with_intercept = np.hstack((np.ones((len(X), 1)),X))\n",
    "B = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ (X_with_intercept.T @ y.reshape(-1, 1))\n",
    "\n",
    "print(\"Manual weights: \", B.reshape(-1))\n",
    "print(\"Manual score: \", r2_score(y, (X_with_intercept @ B).reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5713482748283873, 0.5713482748283873)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "RSS = (((X_with_intercept @ B).reshape(-1) - y)**2).sum() # Squared distance from our new regression line\n",
    "TSS = ((y.mean() - y)**2).sum()                           # Squared distance from the mean\n",
    "r2 = 1 - RSS / TSS                                        # How much distance did we gained ? Did we reduce the errors ? Are we closer to the actual point values ?\n",
    "r2, r2_score(y, (X_with_intercept @ B).reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare those results with sklearn linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sklearn weights:  [-0.07556543642855085, 0.19769138728528463, -1.560875734209466, 1.3223401715433833, -2.5761040065353327, 1.5951628411047127, 1.4360657609756604]\n",
      "Sklearn score:  0.5713482748283873\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = LinearRegression().fit(X, y)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Sklearn weights: \", [lr.intercept_] + lr.coef_.tolist() )\n",
    "print(\"Sklearn score: \", r2_score(y, lr.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with regularization (Ridge regression) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is the action of adding to the loss, a term that contains the weight values. \n",
    "That way these terms are forced to stay small. This helps avoiding overfitting. \n",
    "\n",
    "Let's look at the ordinary least sqaure loss and then add the square of each weight to build the regularized loss. \n",
    "Adding the square of each weight means we buil the Ridge regression loss. If we add the absolute value of each weight we build the Lasso regression loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-4.07656752, -4.10378391, -4.11533025, ..., -4.15223732,\n",
       "        -4.11553644, -4.13368069]),\n",
       " array([[-0.1053435 , -0.13255988, -0.14410623, ..., -0.18101329,\n",
       "         -0.14431241, -0.16245667]]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loss = (((X_with_intercept @ B) - y.reshape(-1, 1)).T  @  (X_with_intercept @ B) - y.reshape(-1, 1)).reshape(-1)\n",
    "regularized_loss = loss + 0.3 * B.T @ B\n",
    "loss, regularized_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way adding this loss impacts the formula is the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual weights:  [-0.07457501  0.19926227 -1.4614579   1.30386275 -2.31228351  1.40463349\n",
      "  1.42708759]\n",
      "Manual score:  0.5710213053584052\n"
     ]
    }
   ],
   "source": [
    "scaled_df = (df - df.min()) / (df.max() - df.min())\n",
    "X = scaled_df[['housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']].values\n",
    "y = scaled_df['median_house_value'].values\n",
    "\n",
    "\n",
    "X_with_intercept = np.hstack((np.ones((len(X), 1)),X))\n",
    "\n",
    "I = np.identity(X_with_intercept.shape[1])\n",
    "I[0,0] = 0\n",
    "B = np.linalg.inv(X_with_intercept.T @ X_with_intercept + 0.3 * I) @ (X_with_intercept.T @ y.reshape(-1, 1))\n",
    "\n",
    "print(\"Manual weights: \", B.reshape(-1))\n",
    "print(\"Manual score: \", r2_score(y, (X_with_intercept @ B).reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sklearn weights:  [-0.07457500943073836, 0.19926227134208827, -1.4614578956147966, 1.3038627486538301, -2.3122835137561593, 1.4046334910837222, 1.4270875901070914]\n",
      "Sklearn score:  0.5710213053584055\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = Ridge(alpha=0.3).fit(X, y)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Sklearn weights: \", [lr.intercept_] + lr.coef_.tolist() )\n",
    "print(\"Sklearn score: \", r2_score(y, lr.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypjZf8PDunca"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the logistic regression, we transform the X values in the same way but we add a sigmoid transform at the end in order to map to values between 0 and 1. \n",
    "\n",
    "We can not use the normal form anymore for computing the weights. We have to resort to other techniques like gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "WhcpkJHN94KL"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return  1 / (1 + np.exp(-x)) \n",
    "\n",
    "def log_likelihood(y_hat, y_true):\n",
    "  # Being far away from the correct class is penalized heavily. \n",
    "  return - np.mean( y_true * np.log(y_hat) + (1-y_true) * np.log(1-y_hat) )\n",
    "\n",
    "def gradient_sigmoid(x):\n",
    "  sigmoid(X) * (1 - sigmoid(X))\n",
    "\n",
    "\n",
    "def gradients(X, y, y_hat):\n",
    "    # Loss = y * log(h) + (1 - y) * log(1-h)\n",
    "    # where h = sigmoid(z)\n",
    "    # and z = Xt @ B\n",
    "\n",
    "    # deriv_loss_to_h = y / h - (1-y) / (1-h) = (y - h) / (h * (1 - h))\n",
    "    # deriv_h_to_z = sigmoid(h) * (1 - sigmoid(h))\n",
    "    # deriv_z_to_b = Xt\n",
    "    # Though chain rule, final derivative \n",
    "    # final_derivative = deriv_loss_to_h * deriv_h_to_z * deriv_z_to_b = x * (y - h) = x * (y - y_hat) \n",
    "    dw = (1/len(X)) * (X.T @ (y_hat - y))\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NV_tk5gS8T-B",
    "outputId": "5f6d87ac-84f7-4b81-fd63-544e80bdcac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.6821755251690551\n",
      "loss:  0.46252854158211454\n",
      "loss:  0.4541283502467595\n",
      "loss:  0.4510517001241438\n",
      "loss:  0.4487185438489886\n",
      "loss:  0.4466409644195144\n",
      "loss:  0.44474208933519344\n",
      "loss:  0.4429996946665173\n",
      "loss:  0.4413999204051543\n",
      "loss:  0.43993073757337586\n"
     ]
    }
   ],
   "source": [
    "df['median_house_value_cat'] = (df['median_house_value'] > 150_000).astype(int)\n",
    "\n",
    "scaled_df = (df - df.min()) / (df.max() - df.min())\n",
    "X = scaled_df[['housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']].values\n",
    "y = df['median_house_value_cat'].values \n",
    "\n",
    "X_with_intercept = np.hstack((np.ones((len(X), 1)),X))\n",
    "\n",
    "B = np.random.normal(0, 0.1 ,(7, 1))\n",
    "\n",
    "for i in range(50_000):\n",
    "  y_hat = sigmoid(X_with_intercept @ B).reshape(-1)\n",
    "  if i % 5000 == 0 or i ==0: \n",
    "    print(\"loss: \", log_likelihood(y_hat, y))\n",
    "  deltas = gradients(X_with_intercept, y, y_hat)\n",
    "  B -= 0.3 * deltas.reshape(-1, 1)\n",
    "\n",
    "\n",
    "lr = sklearn.linear_model.LogisticRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a759pzDOEBJ_",
    "outputId": "5f7905f6-01fa-4169-e606-691d859e01a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual weights:  [ -4.74650191   2.09565859 -11.5802056    7.08212211  -3.00163538\n",
      "   8.00035044  18.85028779]\n",
      "Manual score:  0.8215249055925193 0.8514583915758084\n",
      "\n",
      "Sklearn log loss:  0.43866521703018374\n",
      "Sklearn weights:  [-4.385273936252051, 1.9603353158729624, -10.78106293024599, 6.882196980429938, -2.868679885031378, 7.251350300146187, 17.41000987846787]\n",
      "Sklearn score 0.8186773905272565 0.8536949026185817\n"
     ]
    }
   ],
   "source": [
    "print(\"Manual weights: \", B.reshape(-1))\n",
    "print(\"Manual score: \", \n",
    "        precision_score(y, (sigmoid(X_with_intercept @ B).reshape(-1) > 0.5).astype(int) ),\n",
    "        recall_score(y, (sigmoid(X_with_intercept @ B).reshape(-1) > 0.5).astype(int) ),\n",
    "      )\n",
    "print()\n",
    "print(\"Sklearn log loss: \", log_loss(y, (sigmoid(X_with_intercept @ B).reshape(-1))))\n",
    "print(\"Sklearn weights: \", lr.intercept_.tolist() + lr.coef_.reshape(-1).tolist())\n",
    "print(\"Sklearn score\", \n",
    "      precision_score(y, lr.predict(X)),\n",
    "      recall_score(y, lr.predict(X))\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights are not exactly the same but the performances are very similar. This is due to the randomness aspect of training through gradient descent. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "AI basics.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "fd589ee5ced0891cb656c7a70d902ac388d33fb7278304a3342d085a5b2186d0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
